\lhead{\emph{Methodology}} % This is for the header on each page - perhaps a shortened title
\label{chpt:modelling and techniques}

The primary goal of this thesis is to characterise as many CVs as possible, meaning to find the donor mass and radius, white dwarf mass and radius, and the orbital separation between the two bodies. The composition of this model is described in \S\ref{sect:modelling:lightcurve modelling}.
These metrics provide valuable probes of CV evolution \citep{knigge2006}, which is explored in \S\ref{sect:modelling:evolutionary modelling}, with a specific goal of inferring mass loss rates of donor stars from observations, which is a key element of CV evolution that is poorly understood below the period gap (as discussed in \S\ref{sect:introduction:modern AML}).
This chapter describes in detail the two modelling techniques used for this thesis: the characterisation of a CV using multi-band eclipse modelling, and inferring the long-term mass loss rate of a system from its donor properties.
In addition, since the models used in this analysis are computationally expensive and require large parameter spaces, the choice of fitting algorithm is important. The majority of the parameter optimisation done in this work uses a type of Markhov Chain Monte Carlo (MCMC) technique, and this is described in detail in \S\ref{sect:modelling:parameter optimisation of many variables}, before discussing the models themselves.


\section{Parameter optimisation of many variables}
\label{sect:modelling:parameter optimisation of many variables}
Frequently in science, a model must have its input parameters fit to data. For models with few input parameters and well-behaved evaluation metrics (i.e. goodness-of-fit varies smoothly with input parameters), optimisation is relatively easy, but this is often not the case; for example the eclipse modelling portion of this work (\S\ref{sect:modelling:eclipse modelling}) has 18 parameters for a single eclipse, and fitting a full dataset frequently involves fitting  100+ parameters. To make matters worse, the eclipse model is fairly expensive to compute in large numbers, making a full exploration of the parameter space impractical. Rather, we use MCMC.

The MCMC method is now a well-established tool in astronomy. It is robust, efficient when used properly, and yields the probability distribution of the variables being optimised even when the distributions are not well-described by simple functions. This has led to MCMC often being the method of choice when fitting models.
This section provides a working knowledge of MCMC, but for an in-depth introduction and review of the technique and its various sub-types see \citet{sharma2017}.


\subsection{Bayesian analysis}
\label{sect:modelling:Bayesian analysis}
Bayesian inference uses known, `prior' knowledge combined with new information, `data', to derive a better understanding - the `posterior' knowledge - of a model. This somewhat self-evident intuition is formalised as Bayes' Theorem, which calculates the posterior probability, $p(\theta | D, I)$ a set of model parameters, $\theta$, given some observed data, $D$, and background information, $I$.
\begin{equation}
    \label{eqn:modelling:bayes symbolic}
    p(\theta | D,I) = \frac{\mathcal{L}(D | \theta, I) \cdot q(\theta | I)}{p(D|I)}
\end{equation}
Here, $\mathcal{L}(D | \theta, I)$ is the probability of the observed data, given a model and prior information, so is called the likelihood of the data.
$q(\theta|I)$ is the probability of the model being valid, given some prior information, so is called the prior distribution. Finally, $p(D | I)$, or the probability of observing the data, given the previously known information, is also called the `Evidence', and acts as a normalisation factor. Using this vocabulary, Equation~\ref{eqn:modelling:bayes symbolic} can be written as:
\begin{equation}
    \label{eqn:modelling:bayes english}
    \rm Posterior = \frac{Likelihood \times Prior}{Evidence}
\end{equation}
In Bayesian inference, the goal is to find the posterior distribution of the parameters of a model, given some data and, if possible, prior information.


\subsection{MCMC optimisation}
\label{sect:modelling:affine invariant MCMC}
The MCMC technique is a class of tools developed to approximate the posterior distribution in Equation~\ref{eqn:modelling:bayes symbolic}. Analytical calculations of the posterior are predicated on knowing the analytical forms of the likelihood, prior and evidence, which is often not known.

An MCMC sampler, as the name suggests, is a combination of a Monte Carlo method, a class of algorithms that rely on random sampling to find a result, and a Markov chain, a mathematical system that transitions between states according to probabilistic rules \citep{foreman2012}.
An MCMC randomly samples the prior distributions of the model variables (the Monte Carlo half of the algorithm), evaluates their $\mathcal{L}$ and $q$, and either accepts them onto its chain of sampled points or not, depending on if they meet a set of conditions (the Markov chain half of the algorithm). If the $\mathcal{L}$ of the proposed set of variables is higher than the $\mathcal{L}$ of the last set on the chain, the proposed set of variables is accepted. If $\mathcal{L}$ is lower, the algorithm randomly accepts or rejects the proposed step.
Conveniently, $\mathcal{L}$ is usually related to the $\chi^2$ metric by $\mathcal{L} \propto \mathrm{exp}(-\chi^2/2)$, so a change in $\mathcal{L}$ is often relatively easy to compute.
The method by which a new set of parameters is proposed, and acceptance decided in the event of a decrease in $\mathcal{L}$ is the sampling method, and several choices exist for different types of problems. The sampling method used here is the affine invariant sampling method with parallel tempering, described below.

By giving a finite chance to accept a `worse' set of parameters, the chain is, in theory, allowed to explore and sample the entire possible parameter space without becoming trapped in local minima, though this requires an infinitely long chain.
However, as the sampler preferentially accepts positions with higher $\mathcal{L}$, as the length of the chain increases the distribution of samples on the MCMC chain approaches the `true' distribution of the posterior.


\subsubsection{Affine invariant ensemble sampling}
\label{sect:modelling:Affine invariant ensemble sampling}
The affine invariant ensemble method of sampling was developed by \citet{goodman2010}, and makes use of many `walkers' sampling the parameter space in tandem.
Each walker functions as an individual MCMC chain, and the walkers interact by proposing steps based on the current states of other walkers.
A new parameter vector for walker $k$ is proposed via a `stretch move'; another walker, $j$ is chosen at random, and the last position vectors on each chain, $\theta_{j,N}$ and $\theta_{k,N}$, are used to propose a new position, $\Theta_k$, that lies somewhere on the line connecting the two position vectors.
\begin{equation}
    \Theta_k = \theta_j + z \cdot (\theta_k - \theta_j)
\end{equation}
The variable $z$ determines the location of the new vector on the line, and is randomly drawn from a probability distribution $g(z)$,
\begin{equation}
    g(z) \propto
    \begin{cases}
        \frac{1}{\sqrt{z}}, & \frac{1}{2} \le z \le 2 \\
        0, & Otherwise
    \end{cases}
\end{equation}
The choice of $g(z)$ favours a consolidation of the walkers, and aids convergence on regions of high $\mathcal{L}$. A schematic of this stretch move concept is shown in Figure~\ref{fig:modelling:stretch move}.
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{figures/modelling/stretch_move.png}
    \caption{Reproduced from \citet{goodman2010}, showing the concept of a stretch move proposal. The proposed next step for $j$ is given by choosing a random position on the line joining the last step on chain $j$, $X_j$, and the last chain on another walker chosen at random, $X_k$. The proposed step is shown by the star, $Y$. Grey dots with no outlines illustrate the other walkers in the ensemble, but are unused.}
\label{fig:modelling:stretch move}
\end{figure}

Then, $\Theta_k$ is either accepted or rejected from the chain depending on the current state of the chain. Recall that if the proposed $\mathcal{L}_{N+1} > \mathcal{L}_{N}$, i.e. the likelihood has improved, the sample is immediately accepted. However, if $\mathcal{L}_{N+1} < \mathcal{L}_{N}$, the acceptance is determined by the transition probability, $P(\Theta_k | \theta_{k,N})$, defined as
\begin{equation}
    P(\Theta_k | \theta_{k,N}) = \alpha(\Theta_k | \theta_{k,N}) \cdot q(\Theta_k | \theta_{k,N})
\end{equation}
where $\alpha(\Theta_k | \theta_{k,N})$ is the acceptance probability,
\begin{equation}
    \alpha(\Theta_k | \theta_{k,N}) = \mathrm{min}\bigg( 1,\ z^{n-1}\frac{\mathcal{L}(\Theta_k)}{\mathcal{L}(\theta_{k,N})} \bigg)
\end{equation}
for a model with $n$ dimensions.
A random number, $u$, is drawn from a uniform distribution from 0 to 1, and if $u < \alpha(\Theta_k | \theta_{k,N})$, the proposed position is accepted.
This quantifies two aspects of the sampler: the larger the drop in the probability that a new $\Theta_k$ describes observation, the \textit{less likely} the algorithm is to accept $\Theta_k$ onto the chain; and larger stretch moves are more likely to be accepted.
At each step in the MCMC, every walker has a new position proposed this way.

% This finite chance to accept a new point, even if it is less likely to describe the data, makes the chain \textit{reversible} and allows the algorithm to explore the full allowable parameter space (given an infinite number of steps) even if doing so first requires moving to a less preferred $\theta$.
The affine invariant ensemble sampler benefits significantly from having the walkers in the ensemble interact, as they can communicate to other walkers regions of high $\mathcal{L}$ even between walkers in different local minima. Further, the sampler has a higher likelihood to accept dispersing steps than consolidating steps, so walkers are only likely to gather in deep minima, increasing exploration except in the case of a significantly higher likelihood.
This improves the ensemble's ability to both locate the global minimum, and to sample non-spherical probability distributions (a task that can be difficult for simpler sampling techniques).
Further, this algorithm is able to have the proposal and evaluation of new steps in every chain performed \textit{simultaneously}, significantly improving computation time;
a typical rule-of-thumb is to use $2n$ walkers for $n$ parameters being optimised \citet{goodman2010}, and since the number of walkers is almost always larger than the number of available threads, increased evaluation time scales well with more threaded computation.


\subsubsection{Parallel tempering}
\label{sect:modelling:parallel tempering}

In metallurgy, a metal can be toughened by relieving its internal stresses through tempering, a treatment in which a metal is first heated to a high temperature, then slowly cooled.
While the metal is at a high temperature, impurities are able to diffuse throughout the crystal structure of the metal and explore possible crystallisation locations. As the metal slowly cools, impurity atoms are gradually more and more attracted to areas of the crystal that exert less stress on the material, until the metal is fully cooled and the majority of atoms have found areas of local minima in stress potential.

The ensemble MCMC can take analogy from this `hot' exploration phase and `cool' settling phase \todo{I need a reference for parallel tempering...}. This is done by running several parallel ensembles, that each have a different `temperature' between 1 and $\infty$. Each ensemble samples a modified posterior, that follows $\pi_T(\Theta)$;
\begin{equation}
    \pi_T(\Theta) = [\mathcal{L}(\Theta)^\frac{1}{T}] q(\Theta)
\end{equation}
As $T \rightarrow \infty$, the chain samples the prior with no respect to how well $\Theta$ describes the data. This hot chain is analogous to the diffusive atoms with much higher thermal energy than the stress potential of the metal, and is free to randomly explore available parameter space without being restricted by the $\mathcal{L}$ function, potentially finding regions of high likelihood far from the initial conditions and communicating these regions back to cooler ensembles. Cooler temperatures are analogous to the cooling metal -- drawn increasingly strongly towards regions of high likelihood.
The cold case of $T \equiv 1$ behaves equivalently to a normal ensemble sampler.

Note that because hotter walkers are less sensitive to $\mathcal{L}$, their posterior no longer accurately reflects the `true' distribution. If a parameter is described by a Gaussian distribution with a standard deviation, $\sigma$, the tempered $\mathcal{L}$ will have a standard deviation of $\sigma \sqrt{T}$.

When running a parallel tempered MCMC, the values used in the software are $\beta = 1/T$, for which we choose between 3 and 5 evenly spaced values of $\beta$ between 0 and 1.
The number of temperatures used depends on the evaluation time; since parallel tempering runs multiple full ensembles in tandem, in models with very large numbers of parameters it becomes highly desirable to use as few temperatures as possible. However, the penalty in computation time per step comes with the benefit of a dramatically improved ability to locate the global minimum, especially in complex parameter space.



\section{Finding an orbital ephemeris}
\label{sect:modelling:getting ephemeris}

Crucial to both observing and modelling an eclipse is a good knowledge of the orbital ephemeris. This is described by the equation
\begin{equation}
    \label{eqn:modelling:general ephemeris equation}
    T_{\rm ecl} = T_0 + P_{\rm orb} E
\end{equation}
where $T_{\rm ecl}$ is the time of mid-eclipse, $T_0$ is the mid-eclipse time of the zeroth eclipse, and $E$ is the eclipse number. Accurately calculating $T_{\rm ecl}$ is important to scheduling observations of a system, and $P_{\rm orb}$ is a crucial to the eclipse modelling.

As observations are often separated by several months or even years, an error in $P_{\rm orb}$ of even $\sim 0.1$ seconds can accumulate to give significantly inaccurate predicted eclipse times. This need for precision also requires the definition of {\it where} a time is recorded from, as the delay introduced by the light travel time from one side of the earth's orbit to the other can significantly offset an observed time. All eclipse times presented in this thesis are given in the Barycentric Modified Julian Date (BMJD), which is the time of eclipse as measured from the centre of mass of the solar system. Note that this is different to the heliocentric MJD often seen in the literature, and where heliocentric literature values are used, they are converted to BMJD.

\subsection{Finding eclipse times}
\label{sect:modelling:finding eclipse times}

When finding an eclipse time, simply taking a time of minimum light is insufficient for the systems in this work. This is because it is common for CV eclipses to have very flat eclipse minima, and because CV eclipses have a fairly complex structure.
Rather, finding the mid-eclipse time is done by looking at the numerical derivative of an eclipse.
First, an eclipse is smoothed to remove short term fluctuations, partly those due to noise but also to mitigate the short term flickering often seen in CVs. This initial smoothing is done by applying a median filter to the data. Then, the numerical derivative is calculated and smoothed again, this time with a `boxcar' convolution (a.k.a. a moving average). Properly filtered, the dominant remaining features of the numerical derivative are the ingresses and egresses of the white dwarf and bright spot.
The white dwarf ingress and egress are, in theory, symmetrical -- the ingress should be a sharp, negative spike, and egress should be a sharp, positive spike. As the two should be the same shape, a double-Gaussian is fit to the derivative, using manually chosen initial conditions. In this model, two Gaussians share a width, $\sigma$, and have their mid-points equidistant from a central point. The magnitude of their heights are shared, but with opposite signs;
\begin{align*}
    T_{\rm 1,2} = T_{\rm ecl} \pm \Delta T \\
    h_{\rm 1,2} = \pm h
\end{align*}
where $T_{1,2}$ are the respective midpoints of the two Gaussians, $h_{1,2}$ are their respective heights, and $2\Delta T$ is the distance between the two Gaussians.
The derivative is then fit with these four free parameters ($T_{\rm ecl},\ h,\ \Delta T,\ \sigma$) using an MCMC with wide, uniform priors of appropriate ranges, to give the $T_{\rm ecl}$.

\subsection{Computing period}
\label{sect:modelling:Computing ephemeris}

To find a rough initial ephemeris of a system, at least two eclipse observations with known $E$ are necessary. Given no prior knowledge of $P_{\rm orb}$ and $T_0$, this can be done by simply observing the system for several hours, until two consecutive eclipses are seen. This gives a rough measure of $P_{\rm orb}$, but can be significantly refined with longer baseline observations.
For each observed $T_\mathrm{ecl}$, $E$ could unambiguously be determined, either from observing consecutive eclipses or from previously reported literature values. Where literature values were used to calculate a value of $E$, the result never deviated from an integer by more than 0.25 and were rounded to the nearest whole number.

An MCMC algorithm was used to fit a straight line model to the independent variable $E$\ and dependent variable $T_\mathrm{ecl}$, with a gradient $P$\ and intercept $T_0$; i.e. model values of $T'_{\rm ecl}$ were generated from the set of $E$ and a proposed $(P, T_0)$ pair, and $(T_{\rm ecl} - T'_{\rm ecl})$ was minimised. Again, wide uniform priors were used for $P$ and $T_0$, based on initial values.

The model also accounts for potential systematic differences in timing accuracy between instruments by having variable error scale factors applied to all eclipses observed with a specific instrument. For example, the timing reported for eclipses observed with ULTRACAM may be systematically offset from reality, and the errors associated with those observations might need to be larger than reported to be consistent with data from other instruments. The prior distribution assumed for these error factors was log-uniform ranging from 0.01 to 5, which favours the smallest error-multiplying factor consistent with the data.

Finally, the values of $E$ for each eclipse were offset to minimise the covariance between $T_0$ and $P$.
Consider a predicted eclipse time for $E$. The uncertainty on $T_{\rm ecl}$ in Equation~\ref{eqn:modelling:general ephemeris equation} can be written as,
\begin{equation}
    \label{eqn:modelling:error in ephemeris}
    \sigma_{T{\rm ecl}}^2 = \sigma_{\rm T0}^2 + 2\sigma_{\rm T0} \sigma_{\rm P}E + \sigma_{\rm P}^2 E^2
\end{equation}
from the standard error propagation formula.
To evaluate an alternative set of $E'$, $E$ can be offset by some integer, $N$, with $E' = E - N$. By substituting this into equation~\ref{eqn:modelling:error in ephemeris}, expanding out the brackets, and consolidating some terms, $\sigma_{T{\rm ecl}}^2$ becomes,
\begin{align*}
    \sigma_{T{\rm ecl}}^2 =& \sigma_{\rm T0}^2 + 2\sigma_{\rm T0} \sigma_{\rm P}(E'+N) + \sigma_{\rm P}^2 (E'+N)^2 \\
    % =& (\sigma_{\rm T0}^2 + \sigma_{\rm P}^2 N^2 + 2\sigma_{\rm T0} \sigma_{\rm P} N) + 2(\sigma_{\rm T0} \sigma_{\rm P} + \sigma_{\rm P}^2 N)E' + \sigma_{\rm P}^2 E'^2
\end{align*}

To minimise the cross-correlation between $P$ and $T_0$, the second term of the above equation should be minimised.
This is achieved by setting $N = -(\sigma_{\rm T0}\sigma_{\rm P})/(\sigma_{\rm P}^2)$, and re-fitting the ephemeris. Then, the above becomes:
\begin{equation}
    \sigma_{T{\rm ecl}}^2 = \sigma_{\rm T0}^4 + (\sigma_{\rm P}E')^2
\end{equation}
with no cross correlation, in theory. In practice, as $E$ must be rounded to an integer, some residual cross-correlation persists, but is minimised.

\newpage
\section{Characterising a CV}
\label{sect:modelling:lightcurve modelling}

\begin{table}
    \centering
    \caption{The various symbols used in this chapter, and their meanings.}
    \label{table:modelling:parameter key}
    \begin{tabular}{cc}
        \hline
        Symbol & Parameter \\
        \hline
        \hline
        $F_{\rm wd,\ donor,\ disc, bs}$                                 & White dwarf, donor star, disc, and bright spot fluxes   \\
        $T_{\rm eff}$                                                   & White dwarf effective temperature \\
        log($g$)                                                        & White dwarf surface gravity \\
        $M_{\rm wd}$, $R_{\rm wd}$                                      & White dwarf mass and radius                             \\
        $M_{\rm donor}$, $R_{\rm donor}$                                & Donor star mass and radius                              \\
        $q$                                                             & Mass ratio                                              \\
        $a$                                                             & Orbital separation                                      \\
        $x_{l1}$                                                        & Distance from the white dwarf to the $L_1$ point        \\
        $i$                                                             & inclination                                             \\
        $\Delta \phi$                                                   & White dwarf eclipse width in units of phase             \\
        $R_{\rm disc}$                                                  & Accretion disc radius                                   \\
        $b$                                                             & Disc surface profile exponent                           \\
        $\theta_{\rm yaw}$, $\theta_{\rm tilt}$, $\theta_{\rm az}$      & Bright spot yaw, tilt, azimuth                          \\
        $S$                                                             & Bright spot length scale                                \\
        $Y, Z$                                                          & Bright spot profile exponents                           \\
        $u_{\rm ld}$                                                    & White dwarf limb darkening coefficient                  \\
        $\phi_0$                                                        & An eclipse phase offset                                 \\
        $\pi$                                                           & Parallax                                                \\
        E(B-V)                                                          & Interstellar extinction   \\

        \hline
    \end{tabular}
\end{table}

To determine the system parameters for the CVs in this study, the eclipse light-curves were modelled. This method is more frequently applicable in CVs than the more traditional approach of using spectroscopic eclipsing binaries, since the donor star is rarely directly visible, and compared to using the superhump period excess to estimate the mass ratio \citep{patterson2005, knigge2006}, lightcurve modelling requires few assumptions. However, it does require reasonably precise alignment of the system and so is not possible for a large fraction of CVs.

CV eclipse modelling was first developed by \citet{wood1986}, and has been refined significantly over the last decade \citep{Savoury2011, littlefair2014, McAllister2017, McAllister2019}. The method relies on four assumptions, namely that: \textit{(1)} the stream of mass flowing from the donor to the white dwarf follows a ballistic trajectory, \textit{(2)} the white dwarf obeys a theoretical mass-radius relationship, \textit{(3)} the white dwarf is unobscured by the accretion disc or other sources of intra-system material, and \textit{(4)} the donor exactly fills its Roche lobe.
Most of these assumptions are considered robust, though the visibility of the white dwarf been called into question by \citet{Spark2015}.
The white dwarf mass-radius relationship was recently tested by \citet{parsons2017}, and found to be a reasonable assumption.
Assuming that the mass stream following a ballistic trajectory appears to be a reasonable assumption, as the thermal velocity of the donor surface is orders of magnitude lower than the orbital velocity of the two stars.
However, the most convincing argument to the validity of these assumptions are comparative studies, showing good consistency between eclipse modelling and other techniques \citep{tulloch2009,copperwheat2012,savoury2012}.

The rough outline of the modelling process is described here, but is detailed fully in \S\ref{sect:modelling:eclipse modelling}. Throughout, symbols are typically used when referring to model and system parameters, and a key is provided in Table~\ref{table:modelling:parameter key}
Radii are found by assuming that the secondary star completely fills its Roche lobe, which is required for mass transfer and ensures that the donor radius is solely a function of mass ratio, $q$, and orbital separation, $a$, c.f.~Equation~\ref{eqn:introduction:eggleton approximation}. The white dwarf eclipse width is set by the width of the donor, $a$, inclination, $i$, and $q$ \citep{bailey1979}.

Assuming that the mass stream between the two stars follows a ballistic trajectory puts the stream on a calculable path, determined by $q$ \citep{Lubow1975}. This allows the location of the bright spot to be fixed in space, as the point at which this path intersects the outer edge of the accretion disc. Therefore, the phase of the bright spot ingress and egress is a function of $q$, and disc radius.

By assuming that the white dwarf is unobscured, the duration of white dwarf ingress and egress are dependent on the white dwarf radius and inclination.

Four components of the eclipse model are the four component fluxes, $F_{\rm wd}$, $F_{\rm donor}$, $F_{\rm disc}$, $F_{\rm bs}$.
As the white dwarf is assumed to follow a known mass-radius relationship, by fitting the observed white dwarf colours with a temperature, gravity, distance and interstellar extinction, the temperature and radius of the white dwarf yield a mass. The donor mass is then a simple product of the white dwarf mass, and $q$.
The final result of modelling are then the following system parameters:
\begin{itemize}
    \setlength\itemsep{0em}
    \item white dwarf and donor masses
    \item white dwarf and donor radii
    \item orbital separation
    \item orbital velocity of the white dwarf and donor
    \item inclination
    \item white dwarf effective temperature and surface gravity
    \item distance
\end{itemize}

Practically, the modelling actually takes place in two phases, which are each described in detail. First the phase-folded eclipse is modelled under the above assumptions using proxy variables, then the resulting proxy variables are converted to physical parameters once observations are well-described by an eclipse model. This proxy variable fitting is done for the sake of computational efficiency.

Note that this model requires simultaneously fitting many variables simultaneously, thus finding the best-fitting parameters to observed data is complex. The technique used is described in \S\ref{sect:modelling:optimising eclipse model parameters}.

\subsection{Phase-folded eclipse modelling}
\label{sect:modelling:eclipse modelling}

Recall that the light from a CV originates from four distinct objects in the system. The white dwarf and donor star, the accretion disc about the white dwarf, and the bright spot impact region (hereafter simply `the bright spot'), where transferred material impacts the outer rim of the accretion disc and liberates significant amounts of energy. Notably the bright spot emits flux directionally, so beaming must also be accounted for in the model.
The anatomy of a CV eclipse lightcurve is a sequence of five events, that occur in the following order:
\begin{enumerate}
    \setlength\itemsep{0em}
    \item a pre-eclipse hump is often seen as the bright spot rotates to point at the observer
    \item The white dwarf becomes obscured by the donor
    \item The bright spot becomes obscured by the donor
    \item The white dwarf emerges from behind the donor
    \item The bright spot emerges from behind the donor
\end{enumerate}
Figure\todo{Make a figure for this} shows a typical lightcurve, with these events marked.

At this stage of modelling, parameters are not yet in SI units.
Rather, the phase-folded lightcurve is generated, causing parameters to be scaled. $q$ is fit instead of masses, and distances are scaled to the distance from the white dwarf primary to the $L_1$ point, $x_{l1}$. Distance to the system is also not yet accounted for, so apparent flux is used rather than luminosity.
A single eclipse is described by 18 parameters:
\begin{itemize}
    \setlength\itemsep{0em}
    \item White dwarf, donor star, disc, and bright spot fluxes, $F_{\rm wd,\ donor,\ disc, bs}$
    \item Mass ratio, $q$
    \item White dwarf eclipse width in units of phase, $\Delta \phi$
    \item Scaled white dwarf radius, $R_{\rm wd} / x_{l1}$
    \item White dwarf limb darkening coefficient, $u_{\rm ld}$
    \item Scaled outer disc radius, $R_{\rm disc} / x_{l1}$
    \item Disc surface profile exponent, $b$
    \item Seven parameters describing the bright spot
    \item An eclipse phase offset, $\phi_0$
\end{itemize}
The seven bright spot parameters are not physically motivated, but describe a flexible empirical bright spot model designed to capture a large range of bright spot eclipse morphologies.

\subsubsection{The white dwarf}

The white dwarf is modelled as a luminous disc, with a total surface brightness $F_{\rm wd}$ and a radius of $R_{\rm wd} / x_{l1}$. It is subject to limb darkening, using a linear prescription:
\begin{equation}
    \frac{I_l}{I_0} = 1 - u_{\rm ld}(1 - {\rm cos}\beta)
\end{equation}
where $I_0$ is the intensity at the centre of the disc, and $I_l$ is the intensity at a limb. $\beta$ is the angle between the line normal to the surface of the white dwarf, and the observer's line of sight.
However, the observations are not precise enough to constrain $u_{\rm ld}$, and this is functionally a free parameter.

\subsubsection{The donor star}

The secondary star does not become obscured during an eclipse, but there is still some variation in its brightness. The donor is not spherical, so a small ellipsoidal variation is seen as it rotates to expose more or less of its surface to the observer. As a result, the donor is modelled as a uniformly bright disc with total surface brightness $F_{\rm donor}$, and a modulation to describe the changing observable surface area.

\subsubsection{The accretion disc}

The accretion disc is modelled as a series of annular rings about the white dwarf, extending out to $R_{\rm disc} / x_{l1}$ and with a total surface brightness of $F_{\rm disc}$. The intensity of each annulus decreases with distance from the white dwarf, following an exponential formula, $I_i \propto R^{-b}$ for ring $i$ at distance $R$ from the white dwarf. As $b$ is a free parameter in the model, the disc brightness can be made more or less centrally concentrated to match observations. As the bright spot location is determined by $q$ and $R_{\rm disc} / x_{l1}$, the phases of bright spot ingress and egress provide a valuable constraint for $R_{\rm disc} / x_{l1}$.

\subsubsection{The bright spot}

The bright spot model is not physically motivated, but rather is chosen to be able to reproduce a large range of bright spot eclipses. The only pertinent detail is its times of ingress and egress.
It is modelled as a strip of flux extending in both directions tangentially from the edge of the disc, with a defined brightness profile and overall flux. The strip intensity falls off exponentially, described by the equation
\begin{equation}
    I_{\rm X} \propto \bigg ( \frac{X}{S} \bigg )^Y \cdot {\rm exp} \bigg [ - \bigg (\frac{X}{S} \bigg )^Z \bigg]
\end{equation}
where $I_{\rm X}$ is the intensity of the strip a distance $X$ along it, and $S$ is the scale of the bright spot. $Y$ and $Z$ are the profile exponents.

The bright spot is known to emit light directionally, at a beaming angle, $\theta_{\rm yaw}$, from the normal to the strip in the plane of the disc, and an angle $\theta_{\rm tilt}$ from the plane of the disc.
Some fraction of the light is beamed, and the rest, $f_{\rm is}$, is emitted isotropically from the strip. This geometry is shown in Figure~\ref{fig:modelling:bright spot schematic}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/modelling/bright_spot_schematic.pdf}
    \caption{Showing a schematic of the bright spot model. The {\bf lower dashed line} joins the centres of the white dwarf and donor stars, and the {\bf upper dashed line} runs parallel to it, intersecting the bright spot location. The {\bf straight red line} is one half of the flux-emitting strip and has a profile exponent $Y$, and the {\bf arrow} shows the direction of light emission, at an angle $\theta_{\rm yaw}$ from the normal. The {\bf blue line} shows the other half of the flux-emitting strip, and has the profile exponent $Z$}
    \label{fig:modelling:bright spot schematic}
\end{figure}

Lower values of $q$ will cause the ballistic stream to take a wider arc towards the white dwarf, moving the intersection point with the disc. The angle between the bright spot and disc edge is defined by $\theta_{\rm az}$, the angle between the strip and the line of sight of the observer.

As the bright spot is the most complex component of the model, there is an option to simplify it in software for systems with faint bright spot features that cannot be properly resolved.
This mode is called the `simple' bright spot model, and fixes $\theta_{\rm tilt}$ at $90^\circ$, $\theta_{\rm yaw}$ at $0^\circ$, and the strip exponents $X$ and $Y$ to 1 and 2, respectively. By removing these four degrees of freedom, better and faster characterisation of the eclipse is possible.

\subsubsection{Choice of priors}

The choice of prior is important in Bayesian inference, but this modelling has very little prior knowledge on a system. The prior distributions used were generally uniform and span the numerically allowed range\footnote{e.g.\ angles are limited to be between 0 and $2\pi$, and fluxes range from 0 to the peak flux of the eclipses.}, with a few exceptions.

As it is unconstrained by data, $u_{\rm ld}$ uses a Gaussian prior centred on 0.3 with a width of 0.1.
The bright spot scale draws from a log-uniform prior between 0 and 0.2, which favours smaller values, and $\theta_{\rm az}$ is forbidden from values that would cause the bright spot strip to deviate from a tangent to the disc by $>80^\circ$.
In addition, some combinations of parameters are forbidden in the model. The values of $q$ and $\Delta\phi$ must be such that $i > 90^\circ$ for an eclipse to occur, and the disc radius constrained by the maximum radius before precession becomes a significant effect, $R_{\rm disc} / a < 0.46$.


\subsection{Post-processing the eclipse model}
\label{sect:modelling:post processing the eclipse model}

The eclipse modelling uses proxy variables, so some processing must be done to convert them to `real' values. This is done in two steps. First, a white dwarf temperature and surface gravity are fit to the white dwarf fluxes. Then, the white dwarf temperature and orbital period are combined with the best-fit eclipse model parameters to convert the scaled distances to metres, and mass ratio to the masses of each star, in kilograms.

\subsubsection{Fitting white dwarf colours}
\label{sect:modelling:fitting white dwarf colours}
By modelling the eclipse in multiple bands, at least three observations of white dwarf flux are available.
The DA white dwarf cooling model from \citet{Bergeron1995}\footnote{Available at \href{http://www.astro.umontreal.ca/~bergeron/CoolingModels}{http://www.astro.umontreal.ca/~bergeron/CoolingModels}} is fit to these flux observations.
These cooling models yield the absolute magnitude of the white dwarf in each band, $M$, for a given effective temperature, $T_{\rm eff}$ and surface gravity, $\log (g)$. This absolute magnitude is then easily translated to an apparent magnitude, $m$, given a system parallax, $\pi$, and interstellar extinction coefficient, ${\rm E(B-V)}$,
\begin{equation}
    m = M - 5\log (\pi\mathrm{,\ arcsec}) - 5
\end{equation}

To optimise these four parameters, an affine-invariant MCMC with three levels of parallel tempering was used, c.f. \S\ref{sect:modelling:parallel tempering}. for priors, uniform $T_{\rm eff}$ and $\log (g)$ distributions were used that span the range set by the model cooling tracks. ${\rm E(B-V)}$ used a uniform distribution between 0, and the maximum IRSA measurement for the relevant sky coordinates\footnote{Available at \href{https://irsa.ipac.caltech.edu/applications/DUST/}{https://irsa.ipac.caltech.edu/applications/DUST/}}, and the parallax prior was chosen to match the Gaia measurement of the system \citep{lindegren2018, Luri2018, Gaia2016, Gaia2018}.

\subsubsection{Conversion of proxy variables to physical parameters}
\label{sect:modelling:conversion to physical parameters}
The eclipse model proxy variables are then converted to real values. This is done using the white dwarf as a known quantity, based on white dwarf model atmospheres. Five input variables are needed: $T_{\rm eff}$, $P_{\rm orb}$, $q$, $\Delta \phi$, and $R_{\rm wd} / x_{l1}$.


A measure of the white dwarf radius, $R_{\rm wd}$, can be found using Kepler's 3rd law and making the substitutions $r = R_{\rm wd} / a$ and $q = M_{\rm donor} / M_{\rm wd}$.
\begin{align}
    P_{\rm orb}^2 &= \frac{4 \pi^2 a^3}{G (M_{\rm wd} + M_{\rm donor})} \\[8pt]
    &= \frac{4 \pi^2 R_{\rm wd}^3}{G M_{\rm wd} (1 + q) r^3} \\[8pt]
    R_{\rm wd}^3 &= \frac{P_{\rm orb}^2 r^3 G M_{\rm wd} (1 + q) }{4 \pi^2}
    \label{eqn:modelling:geometric radius}
\end{align}
$r$ can be found from $R_{\rm wd} / x_{l1}$ by calculating $x_{l1} / a$, which itself is a function only of $q$.

Finding $R_{\rm wd}$ this way requires the white dwarf mass. Fortunately, for a given $T_{\rm eff}$ (which is known from the colour fits, \S\ref{sect:modelling:fitting white dwarf colours}), white dwarfs follow tight theoretical mass-radius relationships \citep{parsons2017}, that can be employed to find the unique $M_{\rm wd},\ R_{\rm wd}$ pair that satisfies both Equation~\ref{eqn:modelling:geometric radius} and the theoretical mass-radius relationship.
Specifically, a proposed theoretical mass-radius pair is chosen from a model relationship and a value of $R_{\rm wd, calc}$ is calculated from Equation~\ref{eqn:modelling:geometric radius}. If this matches the theoretical value, the mass is valid. Otherwise, the proposed mass is altered accordingly and a new mass-radius pair is checked again until the two agree.

Three white dwarf mass-radius relations were used. First, a solution was searched for using the \citet{wood1995} models, spanning masses of $0.4 - 1.0 M_\odot$.
If no solution could be found, the \citet{panei2000} models were searched, spanning masses from $0.4 - 1.2 M_\odot$.
Both of these mass-radius relationships account for the white dwarf $T_{\rm eff}$.
If no solution has been found with these two tracks, the \citet{hamada1961} 0 Kelvin mass-radius relation is checked for solutions. This track spans the largest range in mass, from $0.14 - 1.44 M_\odot$. If no solution is found with the \citet{hamada1961} tracks, the model is considered invalid, though this did not occur for any system in this thesis.

Then, the inclination is calculated. $\Delta \phi$ is solely a function of $q$, and $i$. Therefore, we can use the eclipse model values of $\Delta\phi$ and $q$ to calculate the system inclination - this is done by proposing candidate values of $i$, and comparing the calculated $\Delta \phi_{\rm calc}(q,i)$ with the modelled $\Delta \phi$, and adjusting $i$ as needed until the two agree.

Now, three quantities are known; $i$, $M_{\rm wd}$, and $R_{\rm wd}$. As previously mentioned, $R_{\rm donor}$ is assumed to be the Roche radius, from Equation~\ref{eqn:introduction:eggleton approximation}, and $M_{\rm donor}$ is found simply by $(q \cdot M_{\rm wd})$. $a$ is calculated from the two component masses and $P_{\rm orb}$, using Kepler's laws. Finally, the orbital velocities, $K_{\rm wd,\ donor}$ respectively, of the two stars are calculated using Kepler's laws.
\begin{align}
    K_{\rm wd} &= \frac{2\pi a \mathrm{sin}i}{P_{\rm orb}} \frac{q}{1+q} \\
    K_{\rm donor} &= K_{\rm wd} \cdot \frac{M_{\rm wd}}{R_{\rm wd}}
\end{align}


\subsection{Capturing flickering with Gaussian Processes}

CVs almost always display some amount of stochastic variability, known as flickering. Rather than attempting to model this physically, it is instead treated as correlated noise and characterised with a Gaussian process (GP).
The application of GPs to capturing flickering was established by \citet{McAllister2017} based on work by \citet{roberts2012} and \citet{gibson2012}.
The utility of this addition to the eclipse modelling step is a significant improvement in the precision of parameter posteriors, as the GP can be used to subtract flickering from the observations while leaving the key lightcurve features that modelling aims to reproduce, demonstrated by \citet{McAllister2017}.

This section is aimed at giving a working knowledge of GPs in the context of characterising flickering, and for more in-depth discussion the reader is directed towards these works. The mathematics below omits error in flux for legibility, but closely similar derivations are possible that include error when calculating the likelihood of a data set.

\subsubsection{Gaussian Process background}
\label{sect:modelling:GP background}

GPs are a statistical method that can be adapted to produce a series of correlated points across a time (or space) axis, the distribution of which is described by a Gaussian function. The points are related to one another by a joint distribution; to illustrate what a joint distribution is, take the example to two variables $t_1, t_2$, shown graphically in Figure~\ref{fig:modelling:joint distribution}. Each is normally distributed about a central value, but higher values of $t_1$ are more likely to be produced alongside higher values of $t_2$. Thus, knowing the value of $t_1$ can inform the likely value of $t_2$, written as $P(t_1 | t_2)$.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth, clip, trim={0 0 8.5cm 0}]{figures/modelling/joint_distribution.png}
    \caption{Reproduced from \citet{McallisterThesis}. Two variables, $t_{1,2}$, are described by a joint Gaussian distribution. {\bf Blue ellipses} trace lines of equal probability of drawing a sample. {\bf Solid black Gaussians} along each axis show the probability distributions of each variable, and the {\bf dashed black Gaussian} along the y-axis shows the probability distribution of $t_2$, given a fixed value of $t_1$, which is shown by the {\bf vertical dashed line}.}
    \label{fig:modelling:joint distribution}
\end{figure}

By describing a time-series dataset as an arbitrarily large number of variables with a joint distribution between them, the probability of a point being described by a GP, \textit{given the rest of the data}, can be computed. Similarly, the likelihood of an entire data set being described by a GP is also calculable. This principle is the basis of time-series GPs, and allows them to be used when evaluating the goodness-of-fit of a model.

A GP distribution is defined simply by two functions, a mean function, $\mu(t)$, and a covariance function, $k(t_i, t_j)$.
\begin{equation}
    y(t) \sim \mathcal{GP}(\mu(t) \cdot k(t_i, t_j))
\end{equation}
where $t_{i, j}$ are the times of two data points, and are not necessarily adjacent. In this context, the set of $\bf y$ is a set of observed fluxes at times $\bf t$. The distribution of $\bf y$ is then represented by the joint distribution of $P({\bf y} | {\bf t})$, following a multivariate Gaussian, $\mathcal{N}$,
\begin{equation}
    P({\bf y} | {\bf t}) = \mathcal{N}(\mu({\bf t}), {\bf K})
\end{equation}
Where $\bf K$ is the covariance matrix of the multivariate Gaussian, and fully describes how the distribution of each element of $\bf t$ is affected by each other element, forming an $n \times n$ matrix for $n$ data in $\bf t$.
\begin{equation}
    {\bf K} =   \begin{pmatrix}
        k(t_1, t_1) & k(t_1, t_2) & \cdots & k(t_1, t_n) \\
        k(t_2, t_1) & k(t_2, t_2) & \cdots & k(t_2, t_n) \\
        \cdots      & \cdots      & \cdots & \cdots      \\
        k(t_n, t_1) & k(t_n, t_2) & \cdots & k(t_n, t_n) \\
            \end{pmatrix}
\end{equation}

\subsubsection{Computing a covariance matrix}
\label{sect:modelling:GP Kernel choice}

In practice, as $\bf t$ becomes a larger set and $n$ increases, computing the $n \times n$ matrix $\bf K$ becomes impractical. Instead, a kernel is defined that gives analytical functions that approximate each $k(t_i, t_j)$, and the choice of kernel defines the type of correlation between data.

When modelling flickering, a Matern-3/2 kernel is used, which produces a covariance matrix that correlates nearby values more strongly than those further away in time. The kernel has a `memory' timescale, $\lambda$, and an amplitude, $\alpha$, that can be tuned to a data set. This replaces $k(t_i, t_j)$ with $\alpha \cdot k_{M}(r^2)$, where $k_{M}(r^2)$ is defined as
\begin{equation}
    k_{M}(r^2) = \big(1+\sqrt{3r^2}\big) \cdot\exp\big(-\sqrt{3r^2}\big)\ .
\end{equation}
Here, $r^2$ is a pseudo-radius, and is a function of the distance between the two $t_{i,j}$ being considered. Generally, since the Gaussian process technique is applicable to parameter \textit{vectors}, this is written as
\begin{equation}
    r^2 = ({\bf t}_i - {\bf t}_j)^{\top} \cdot \Lambda^{-1} \cdot ({\bf t}_i - {\bf t}_j)
\end{equation}
Note that the choice of the matrix $\Lambda$ defines how other data in a set affect other data, and can be any square matrix of the same width as $\bf t$. For the GP used in this work, a simple $\Lambda$ is used which has $\lambda$ along the diagonal and 0 elsewhere, making $\lambda$ a kernel scale parameter.
In this model each $t_i$ is a single time value, making $\Lambda$ a $1\times 1$ matrix with values of $\lambda$ along the `diagonal' -- functionally, in this GP, $r^2 = \lambda \cdot (t_i - t_j)^2$.

\subsubsection{Evaluating a model fit with a Gaussian process}
\label{sect:modelling:GP model evaluation}

Finally, the $\mathcal{L}$ of a set of ${\bf y}, {\bf t}$ can be calculated given a GP, i.e. $\mathcal{L}(\alpha, \lambda | {\bf y}, {\bf t})$, analogous to $P({\bf y} | {\bf t})$ \citep{rasmussen2006}.
This is the pertinent step to the modelling, as the likelihood function of the data is replaced with the likelihood of the GP.
When evaluating a proposed $\Theta$ in the MCMC, rather than using $\mathcal{L} \propto \mathrm{exp}(-\chi^2/2)$ the likelihood function is replaced with the likelihood of the residuals after observed fluxes have had the eclipse modelled fluxes subtracted, i.e. ${\bf y}_{\rm res} = {\bf y}_{\rm obs} - {\bf y}_{\rm model}$, given an $(\alpha, \lambda)$ pair. This is expressed more clearly algebraically, as
\begin{equation}
    \mathcal{L} = P({\bf y}_{res} | {\bf t}, \alpha, \lambda) = \frac{1}{(2\pi)^{n/2} |{\bf K}|^{1/2}} \exp \bigg(- \frac{1}{2} {\bf y}_{res}^{\top} {\bf K}^{-1} {\bf y}_{\rm res} \bigg)
\end{equation}

One final factor must be accounted for. Flickering appears to be localised to the region of space near the white dwarf, and often reduces in amplitude substantially during the white dwarf eclipse \citep{McAllister2017}.
To capture this in the GP, two kernels are used: one external to the white dwarf eclipse, and one internal to the white dwarf eclipse. Each shares a value of $\lambda$, but has its own freely variable $\alpha$, $\alpha_{\rm out}$ and $\alpha_{\rm in}$.

Overall, the GP adds three new parameters to the eclipse model: $\alpha_{\rm out, in}$ and $\lambda$, which are optimised alongside the eclipse parameters themselves. $\alpha_{\rm out, in}$ use wide, log-uniform priors to prioritise smaller amplitudes. $\lambda$ uses a narrower log-uniform prior, chosen to prevent the timescale from either exceeding the duration of the eclipse, or becoming shorter than the time resolution between data points.
The slightly more complex parameter space that must now be explored, and more computationally expensive evaluation of $\mathcal{L}$, is made up for in a significantly better characterisation of lower quality eclipses \citep{McAllister2017}.



\subsection{Hierarchical model structure}
\label{sect:modelling:optimising eclipse model parameters}

I extend the lightcurve fitting model used by \citet{McAllister2019}, adopting a hierarchical approach to slightly reduce model complexity.

Changes in the disc radius and brightness profile, and bright spot parameters can mean that the same CV has a significantly different eclipse lightcurve at different times, often making it difficult to justify averaging together many eclipses, as features can become smeared out and uninformative. In the worst-case scenario, all 18 parameters would be independently variable for each eclipse, in each band. However, by sharing some parameters between eclipses and bands, this large number of free parameters is slightly reduced, and the posterior of some parameters can be informed by multiple eclipses. \citet{McAllister2017} share $q,\ R_{\rm wd} / x_{l1}$, and $\Delta\phi$ between eclipses, and we extend that concept by organising the model into a hierarchical structure, a schematic of which is shown in Figure~\ref{fig:modelling:hierarchical_model}.

\begin{figure}
    \centering
    \includegraphics[width=.85\columnwidth ]{figures/results/three_cvs_with_weird_colours/GeneralFigs/hierarchical_model_structure.png}
    \caption{The hierarchical structure of the lightcurve model. Parameters are inherited downwards, to produce an eclipse at the `leaves' of the tree, e.g. Eclipse 3 inherits the parameters of Band 2, which in turn inherits the Core parameters. $\mathrm{F_{WD, RS}}$\ represent the fluxes of the white dwarf and donor star, and $\mathrm{U_{LD}}$\ is the limb darkening coefficient of the white dwarf.}
    \label{fig:modelling:hierarchical_model}
\end{figure}

The top level of the model provides the core parameters, which are unchanging between all observing bands and constant across our observations: $q,\ R_\mathrm{WD}/a$, and $\Delta\phi$. We assume the white dwarf and donor fluxes do not change on the timescale of our observations, and so these variables, along with the limb darkening coefficient of the white dwarf, are shared between all eclipses observed with the same filters. The bottom level holds parameters that can vary quickly enough to change between eclipses, i.e. parameters describing the accretion disc and bright spot. By handling parameters this way, we maximise the amount of data informing important variables. We also somewhat reduce the number of free parameters, which aids in model fitting, but the chief justification for the hierarchical approach is that it ensures consistency between eclipses - something not guaranteed when fitting eclipses individually.

Where possible, data were also binned together. Ideally, this has three beneficial effects: the number of eclipses, and therefore the number of parameters, is reduced; binning increases the signal-to-noise ratio of the data; and as the flickering component is not consistent between eclipses, should reduce the degree of flickering present in the data. However, as CV eclipses often have variable bright spot and disc contributions, it is frequently not reasonable to combine eclipses. Varying bright spot and eclipse features will become smeared out when binned, corrupting crucial elements of the eclipse.
This smearing effect is also subtly present, even in data that are consistent. As a bin width must be chosen that will not precisely align with the integration times of the photometric data, binning will always introduce a small blurring of timing data, and whilst this is a small effect, it can significantly alter the white dwarf ingress and egress features.
Therefore, binning is only done when data are sufficiently similar, and is treated with caution.



\section{Evolutionary modelling}
\label{sect:modelling:evolutionary modelling}

Once armed with a robust sample of characterised CVs, evolutionary modelling is able to refine our understanding even further. In \S\ref{sect:introduction:Summary of how AML and Mdot drive period evolution}, I motivated how the donor star inflates in response to mass loss, and how the degree of this inflation is related to the severity of the mass loss.
If the radius of an equivalent star in the absence of mass loss is known, and the observed radius of a CV donor reproduced by stellar structure models by the introduction of some amount of mass loss, the long-term average mass loss rate can be inferred.

The stellar evolution code used is the MESA codebase, and in order to be confident in any mass loss rates derived this way, two key demonstrations of model accuracy are necessary. However, there is \textit{a priori} cause for confidence; largely default MESA configurations are capable of reproducing the canonical \citep{knigge11} donor tracks with impressive accuracy, even reproducing the period gap by a shutdown of magnetic braking \citep{Paxton_2015}.

First, I demonstrate that MESA can closely reproduce the two \citet{knigge11} donor tracks -- recall from \S\ref{sect:introduction:the missing aml problem} that two such tracks are constructed, a `standard' track with only typical gravitational braking below the period gap, and an `optimal' track that amplifies gravitational braking by $2.47\times$.
By default, MESA shuts off magnetic braking when the donor becomes fully convective, a practice which I motivate in \S\ref{sect:introduction:the missing aml problem} to be spurious. Instead, MESA is altered for a fixed magnetic braking cutoff at $0.2 M_\odot$, arbitrarily fixing the donor mass of the period gap in line with \citet{knigge11}.
In addition, \citet{Pala2017a} added a subroutine to MESA that allows for the amplification of gravitational braking below the period gap. This was used to reproduce the `optimal' track. These two changes alone are enough to reproduce the \citet{knigge11} tracks; Figure~\ref{fig:modelling:MESA can reproduce the K11 tracks} shows how closely this MESA model matches. Note that the small deviation at $\sim 0.07 M_\odot$ is due to MESA switching do a different equation of state regime, and is expected.
\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{figures/modelling/compare_K11_withspot_normb_and_1xrmb_fixedcutoff.png}
    \caption{Showing how well MESA can reproduce the canonical \citet{knigge11} donor tracks. {\bf Solid lines} are MESA tracks, and {\bf dotted lines} are the \citet{knigge11} tracks. {\bf Black} lines have only gravitational braking below the period gap, and {\bf red} lines gave gravitational braking at $2.47\times$ strength.}
    \label{fig:modelling:MESA can reproduce the K11 tracks}
\end{figure}

To extract mass loss rates, I will go through three steps.
Firstly, I demonstrate how the radius of a singleton model of a given mass can be tuned to match observations by introducing star spots, and produce a set of M dwarf models that reproduce observations of stars across the CV donor mass range that \textit{aren't} losing mass.
Secondly, I explore the range of masses for which the radius is a reasonable diagnostic for mass loss rate.
Then, I outline the method by which I search for mass loss rates that produce stellar models matching CV donor observations.
% Note that all code used for these steps is publicly available online.\todo{But not until Meridith says so!!!!!!}


\subsection{For what range of masses can we extract mass loss rates?}
\label{sect:modelling:MESA massloss allowable mass range}

It is worth evaluating the feasibility of using the radius to extract mass loss rates. Recall from \S\ref{sect:introduction:period minimum and bouncers} the two timescales that govern the response of the donor to mass loss: $\tau_{\rm KH}$ and $\tau_{\dot M}$. These timescales are calculated by
\begin{align}
    \tau_{\rm KH} \propto& \frac{M_{\rm donor}^2}{L_{\rm donor} R_{\rm donor}} \\[8pt]
    \tau_{\dot M} =& \frac{\dot M}{M_{\rm donor}}
\end{align}

While $\tau_{\rm KH} \ll \tau_{\dot M}$, the donor is able to maintain thermal equilibrium and is indistinguishable from a singleton star of the same mass.
If $\tau_{\rm KH} \gg \tau_{\dot M}$, the donor is \textit{not} able to maintain equilibrium, and mass loss is fast and adiabatic. The donor is inflated by mass loss, but because the star cannot adjust on thermal timescales, the degree of inflation becomes sensitive to the mass loss \textit{history} of the donor.
However, calculating the two timescales for CVs reveals that $\tau_{\rm KH} \sim \tau_{\dot M}$ \citep{knigge11} - meaning that most CV donors are \textit{almost} able to maintain thermal equilibrium, but are still mildly affected by mass loss.
Under this almost-equilibrium regime, mass loss induces some degree of radius inflation in the donor, but because the star is still almost able to adjust on thermal timescales, the degree of inflation does \textit{not} depend on the mass loss history. In this regime, we can discard the mass loss history of the donor, and use the radius inflation as a diagnostic for the long term baseline mass loss rate.

Which regime the donor falls into is mostly a function of $M_{\rm donor}$. As $M_{\rm donor}$ falls, $\tau_{\rm KH}$ begins to rise faster than $\tau_{\dot M}$; Figure~\ref{fig:modelling:how does tauKH and tauMdot vary with donor mass} shows this trend, produced by a MESA model of a CV c.f. \citep{Paxton_2015,Pala2017a}.
The rise in $\tau_{\rm KH}$ relative to $\tau_{\dot M}$ becomes significant at $\sim 0.1 M_\odot$, around the mass the donor enters the fast, adiabatic $\tau_{\rm KH} \gg \tau_{\dot M}$ period bouncer phase c.f. \S\ref{sect:introduction:period minimum and bouncers}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/tau_both_vs_donor_mass_AML000.pdf}
    \caption{Showing how the two timescales, $\tau_{\rm KH}$ and $\tau_{\dot M}$ vary with donor mass below the period gap in CV donors, as modelled by MESA \citep{Paxton_2015,Pala2017a}.}
    \label{fig:modelling:how does tauKH and tauMdot vary with donor mass}
\end{figure}

It would be better to know exactly how much error is introduced by the donor moving out of the $\tau_{\rm KH} \sim \tau_{\dot M}$ regime to draw a more informed mass limit, and this too can be extracted from modelling.
First, a series of donor-like singleton models were evaluated with varying degrees of mass loss rates, uniformly spaced between $\log (\dot M, M_\odot \mathrm{yr}^-1) = -9.5 \rightarrow -11$.
Then, a series of MESA CV models were run with gravitational losses amplified by $x = 1 \rightarrow 6$.
Finally, each model has its radius, $R$, and $\dot M$ extracted at $0.1 M_\odot$. Since the CV models have varying $\dot M$ and the singleton models do not, if $\dot M$ history does not affect radius inflation, the radii between the two sets of models will match. Figure~\ref{fig:modelling:comparing radii at 0.1Msun} shows this, and little divergence between the two sets of radii is visible. However, note that higher rates of mass loss do show a small degree of divergence.
\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{figures/modelling/compare_0.1Msun_with_CV_track_K11_fig1.pdf}
    \caption{Showing the radius and mass loss extracted from MESA models at $0.1 M_\odot$. The {\bf black} line is a series of singleton models with constant mass loss, and the {\bf red} line is a series of CV models with gravitational AML amplified by $x = 1 \rightarrow 6$.}
    \label{fig:modelling:comparing radii at 0.1Msun}
\end{figure}

By plotting the difference between the two sets of models, and repeating the same process for a range of masses, Figure~\ref{fig:modelling:comparing radii over a range of masses} is produced. Now, by looking at what level of divergence historical changes in $\dot M$ induces, we can evaluate what mass range is acceptable. The upper limit on mass must be $0.2 M_\odot$, as this is the enforced mass of the period gap, and for a lower limit I impose an acceptable level of disagreement of $3\%$. It can be seen that the minimum acceptable mass is then $0.08 M_\odot$.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/compare_multiple_mass_with_CV_K11_fig1a.pdf}
    \caption{The inflation of CV model radii, $R_{CV}$, over singleton model radii, $R_S$, from Figure~\ref{fig:modelling:comparing radii at 0.1Msun}, for a range of masses. The {\bf red dashed line} shows the upper limit for acceptable disagreement, and the {\bf black dashed line} shows perfect agreement.}
    \label{fig:modelling:comparing radii over a range of masses}
\end{figure}


\subsection{Modelling star spots in MESA}
\label{sect:modelling:starspots in MESA}

A general problem in the modelling of low mass stars is an under-estimation of their radii, and MESA is no exception. Recently, \citet{BrownPrep}\todo{Get this reference. Can't be long now!} characterised a sample of $\sim 15000$ M dwarfs in detached binaries from \citet{parsons2018} with a 5th order polynomial, referred to as the Brown relation. We use this empirical mass-radius relation as the baseline `zero mass loss' benchmark radius for comparison against models.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/MESA_inflation_over_brown.pdf}
    \caption{Showing the radius inflation of default MESA models over the Brown relation, i.e. $(R_{\rm MESA} - R_{\rm brown}) / R_{\rm brown}$. The horizontal dashed line is the target radius inflation for MESA models. Crosses correspond to MESA models at an age of 2~Gyrs.}
    \label{fig:modelling:MESA inflation over Brown relation}
\end{figure}
The difference between singleton MESA models with no mass loss and the Brown relation is shown in Figure~\ref{fig:modelling:MESA inflation over Brown relation}, however the goal is not to reproduce the Brown relation exactly.
CV donors are filling their Roche lobes, so are non-spherical. To account for this, \citet{knigge11} introduces a $4.5\%$ radius inflation over isolated stars, and I mirror this approach. Thus, the target radius inflation in Figure~\ref{fig:modelling:MESA inflation over Brown relation} is $4.5\%$.

To achieve this baseline inflation, I introduce star spots into MESA. Star spots are magnetic phenomena, where the magnetic pressure from concentrations of magnetic field lines provides partial pressure support to the photospheric material, and since spots must remain in pressure equilibrium with the spotted surface, the temperature in a spotted region is reduced by ideal gas laws. As a consequence, the cooler spotted regions emit less black body radiation, inhibiting energy flux out of the stellar interior, and inflating the star.

\subsubsection{The star spot model}
\label{setc:modelling:star spot model description}

As MESA is a one-dimensional code and star spots are a two-dimensional phenomenon, spots are modelled using the formulation given by \citet{sommers2015}, based on work by \citet{spruit1986}.
Under this spot treatment, two effects are implemented: the photosphere is made inhomogeneous by the presence of spots, and convection is inhibited by the presence of a strong magnetic field.
In the \citet{sommers2015} formulation, the former effect is enforced by altering the effective temperature to a surface-weighted average of the spotted and unspotted surface, and the latter by augmenting the radiative gradient of the star.

The spots cover a fraction of the stellar surface, $f_{\rm spot}$, and a temperature contrast of $x_{\rm spot} = T_{\rm  spot} / T_{\rm amb}$, where the effective temperature of the spotted surface is $T_{\rm spot}$, and the effective temperature of the ambient, unspotted surface is $T_{\rm amb}$. The surface-weighted average of the star, $T_{\rm av}$ is then,
\begin{equation}
    \label{eqn:modelling:surface weighted average temp}
    T_{\rm av}^4 = (1-f) T_{\rm amb}^4 + f_{\rm spot} T_{\rm spot}^4
\end{equation}
And the altered luminosity, $L_{\rm av}$, becomes
\begin{align}
    L_{\rm av} =& 4\pi R^2 \sigma_{boltz} T_{\rm amb}^4 (1 - f_{\rm spot} + f_{\rm spot} \cdot x_{\rm spot}^4) \\
    L_{\rm av} =& 4\pi R^2 \sigma_{boltz} T_{\rm amb}^4 \alpha_{\rm spot} \\
    L_{\rm av} =& L_{\rm amb} \alpha_{\rm spot} \\
\end{align}
$\alpha_{\rm spot}$, the redistribution parameter, and is what actually alters the structure of the star. $\alpha$ is analogous to the blocking area of perfectly black spots, or spots that are completely supported by magnetic pressure.

MESA performs a lookup for the surface pressure from $T_{\rm eff}$ using precalculated boundary condition tables (see \citet{paxton2010,paxton2011} for details). I modify the MESA code to perform this lookup with $T_{\rm av}$.

However, in MESA $T_{\rm eff}$ is not used in the stellar model interior - rather, it uses energies and pressures to calculate structure. Therefore, I alter the above equation to use pressure instead.
Recall the ideal gas equation, for gas in the ambient, unspotted surface. This gas will have pressure and temperature $P_{\rm gas, amb}, T_{\rm gas, amb}$, density $\rho$, and mean molecular weight $\mu$. Here, $R$ denotes the gas constant.
\begin{equation}
    P_{\rm gas, amb} = \frac{\rho R}{\mu} T_{\rm gas, amb}
\end{equation}
The spotted and unspotted surfaces are under pressure (and therefore also density) equilibrium, but the spotted surface pressure has a contribution from gas pressure, $P_{\rm gas, spot}$, and some contribution from magnetic pressure, $P_{\rm mag, spot}$. Therefore, we can write
\begin{align}
    P = P_{\rm gas, amb} =& P_{\rm gas, spot} + P_{\rm mag, spot} \\
    \frac{\rho R}{\mu} T_{\rm amb} =& \frac{\rho R}{\mu} T_{\rm spot} + P_{\rm mag, spot} \\[12pt]
    P_{\rm mag, spot} =& \frac{\rho R}{\mu} (T_{\rm amb} - T_{\rm spot}) \\
    % P_{\rm mag, spot} =& \frac{\rho R}{\mu} (1 - x_{\rm spot}) T_{\rm amb} \\
    P_{\rm mag, spot} =& (1-x_{\rm spot}) P_{\rm gas, amb} \\
\end{align}
And therefore,
\begin{align}
    P_{\rm gas, spot} =& P_{\rm gas, amb} - P_{\rm mag, spot}\\
    P_{\rm gas, spot} =& P_{\rm gas, amb} - (1-x_{\rm spot}) P_{\rm gas, amb} \\
    P_{\rm gas, spot} =& x_{\rm spot} P_{\rm gas, amb}
\end{align}
However, star spots do not penetrate to the core of the star. To quantify this, rather than fixing $x_{\rm spot}$ and calculating the new pressure at each depth of the star, I calculate the pressure differential at the surface of the star, and fix this pressure difference for interior layers. As the pressure rises with depth, a significant pressure difference at the surface quickly becomes insignificant. For each depth of the star, $x_{\rm spot}$ is calculated from its total pressure before any alteration is made, $P_i$, and the surface pressure, $P_{\rm surf, amb}$,
\begin{align}
    P_{\rm gas, amb} - P_{\rm mag, spot} =& x_{\rm spot} P_{\rm gas, amb} \\
    x_{\rm spot, i} =& \frac{P_i - P_{\rm surf, amb}}{P_i}
\end{align}
Rather than directly altering the pressure profile of the star, the radiative gradient, $\nabla_r$, at each depth is modified. MESA then uses $\nabla_r$ to compute a self-consistent pressure profile for the star.
\begin{align}
    \alpha_{\rm spot, i} =& 1 - f_{\rm spot} + f_{\rm spot} \cdot x_{\rm spot, i}^4 \\
    \nabla_{r,i}' =& \frac{\nabla_{r,i}}{\alpha_{\rm spot, i}}
\end{align}
Values of $f_{\rm spot}$ and $x_{\rm spot}$ can be passed to MESA as user-configured parameters to define the degree of spotting. Figure~\ref{fig:modelling:spotted model radii at 2Gyrs} shows the radii of main sequence stellar models at 2~Gyrs, and $0.15 M_\odot$ with progressively more spots.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/spotted_model_radii_at_2gyrs.pdf}
    \caption{Showing how model radius varies as a function of spot coverage. Here, spots are perfectly black ($x_{\rm spot} \equiv 0$), and the radius is extracted at 2~Gyrs. Evaluated MESA models are shown as {\bf black crosses}, and joined by a {\bf black line} to guide the eye.}
    \label{fig:modelling:spotted model radii at 2Gyrs}
\end{figure}


\subsection{Tuning star spot parameters to observations}
\label{sect:modelling:tuning star spots to observations}
With star spots implemented in MESA, the Brown relation can now be reproduced.
$x_{\rm spot}$ is fixed at 0 and $f_{\rm spot}$ is varied.
Since radius increases monotonically with $f_{\rm spot}$, a binary chop is performed, optimising for $\Delta R = R_{\rm MESA} - R_{\rm Brown} = 0$ at a stellar age of 2~Gyrs.

To find the appropriate $f_{\rm spot}$ for a given mass, the following binary chop steps are followed:
\begin{enumerate}
    \setlength\itemsep{0em}
    \item First, MESA models using the upper and lower bounds of $f_{\rm spot}$ ($f_{\rm up}$ and $f_{\rm low}$ respectively) are evaluated, and $\Delta R$ calculated for each.
    \begin{itemize}
        \item The lower bound will have $\Delta R < 0$, and the upper bound will have $\Delta R > 0$, and since $\Delta R$ is a smooth function of $f_{\rm spot}$, $\Delta R \equiv 0$ must lie between them.
    \end{itemize}
    \item Then, $\Delta R$ at the midpoint, $f_{\rm mid} = (f_{\rm up} + f_{\rm low})/2$, is evaluated, and accepted as either the new $f_{\rm up}$ (if $\Delta R$ is positive) or $f_{\rm low}$ (if $\Delta R$ is negative).
    \item A new $f_{\rm mid}$ is then calculated from the new range, and the cycle repeats until $\Delta R$ is within tolerance; set to $0.1\%$.
\end{enumerate}
The resulting M-$f_{\rm spot}$ relation is shown in Figure~\ref{fig:modelling:fspot mass relationship}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/fspot_relation_to_match_brown_plus_4.5.pdf}
    \caption{{\it Top}: The required $f_{\rm spot}$ that is applied to tune M dwarf MESA models to match the Brown relation, plus an added $4.5\%$ inflation due to non-spherical Roche geometry. {\it Bottom}: the residuals from the best fit value of $f_{\rm spot}$. Note that when finding the necessary value of $f_{\rm spot}$ to match the Brown relation $x_{\rm spot} \equiv 0$, and negative values of $f_{\rm spot}$ were allowed. However, in all subsequent modelling, negative $f_{\rm spot}$ were set to 0. {\bf Red squares} show evaluated MESA models.}
    \label{fig:modelling:fspot mass relationship}
\end{figure}

Below masses of $\sim 0.13 M_\odot$, the required $f_{\rm spot}$ becomes slightly negative, i.e. default MESA models are larger than observations plus the $4.5\%$ non-spherical correction.
Since a negative coverage fraction is unphysical, negative values of $f_{\rm spot}$ are set equal to 0, and we note that derived mass loss rates become more unreliable below this mass.
However, the severity of this unreliability is not catastrophic, as the minimum value of $f_{\rm spot}$ is still reasonably close to 0.

There is significant scatter in the Brown mass-radius relation, that is not captured in these models. The inherent scatter in radius for the observations is \todo{GET THIS} between 0.1 and 0.2 $M_\odot$, which adds to the uncertainty in modelled radius inflation, and thus mass loss rate.
However, while this may skew an individual system, on average the inferred mass loss from model radius should be accurate. Therefore, this effect should not skew the results with a large enough sample size.


\subsection{Optimising mass loss rate to donor observations}
\label{sect:modelling:optimising mass loss rate to observations}

Finally, the mass loss rate required to match donor observations can be found. As the inflation of the donor increases monotonically with increasing mass loss, the binary chop algorithm is used to optimise mass loss, similarly to optimising $f_{\rm spot}$.
To sample the error distribution of observed masses and radii, random samples were drawn from the mass and radius posterior distribution for each observation. The mean and standard deviation of the corresponding mass loss rates is reported.

\subsection{Mass loss from the white dwarf properties}
\label{sect:modelling:mdot from WD temperature}

The white dwarf temperature also reveals information on the mass transfer rate, and the following summary is described more fully by \citet{townsley2009}.
In brief, as this material strikes the surface of the white dwarf the kinetic energy is converted to thermal energy, heating the white dwarf. The degree of this heating is related to the rate at which material falls to the surface -- if more material falls in, more heating is induced.
The short-term average mass loss rate, $<\dot M>$, is ultimately a function only of white dwarf mass, and temperature, given in Equation~\ref{eqn:modelling:Mdot from WD temperature}.
\begin{equation}
    \label{eqn:modelling:Mdot from WD temperature}
    T_{\rm eff} = 1.7 \times 10^4 {\rm K} \bigg( \frac{<\dot M>}{10^{-10} M_\odot {\rm yr}^{-1}} \bigg)^{1/4} \bigg( \frac{M_{\rm wd}}{0.9 M_\odot} \bigg)
\end{equation}

However, the white dwarf temperature is capable of responding to changes in $\dot M$ on $\tau_{\rm TWD} \sim 10^3 - 10^5\ {\rm yrs}$, as opposed to the $\rm \sim Gyr$ timescales of the evolutionary modelling-based method described above, so only provides a short-term snapshot of the $\dot M$.
The white dwarf temperature approach does hold a major advantage, however, in that the `zero $\dot M$' $T_{\rm eff}$ is more well-known and commonly accepted than the `zero $\dot M$' $R_{\rm donor}$ required by this newer method.

Recently, \citet{Pala2021} used spectroscopically measured $T_{\rm eff}$ and $M_{\rm wd}$ to infer the $\dot M$ of 65 CVs, the result of which is reproduced in Figure~\ref{fig:modelling:pala2022 fig13}.
The key finding from this analysis was an inverse correlation between $M_{\rm wd}$ and $\dot M$, contrary to the prediction of gravitational wave braking that lower mass systems should have AML rates, and thus lower $\dot M$.
As the eclipse modelling of CVs produces a measure of $T_{\rm eff}$, the systems analysed for this thesis can be processed with {\it both} techniques, and have their results compared.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/pala_2022_fig13.png}
    \caption{Reproduced from \citet{Pala2021}, Figure~13. The subset of modelled systems, with $P < 3{\rm hr}$ are shown. {\bf Circles} and {\bf stars} are pre- and post-period bounce systems derived by \citet{Pala2021}, and {\bf diamonds} and {\bf pentagons} are pre- and post-period bounce systems taken from the literature. {\it Left}: The $T_{\rm eff}$ is plotted against $M_{\rm wd}$, and no correlation can be seen. {\it Right}: $\log<\dot M>$ is plotted against $M_{\rm wd}$, though now the data are correlated along the white dwarf mass-radius relationship assumed by \citet{Pala2021}, $M_{\rm wd} \propto R_{\rm wd}^1.8$. The {\bf black line} shows the rough relationship, and guides the eye.}
    \label{fig:modelling:pala2022 fig13}
\end{figure}
