\lhead{\emph{Modelling, techniques, and methods}} % This is for the header on each page - perhaps a shortened title
\label{chpt:modelling and techniques}

The primary goal of this thesis is to characterise as many CVs as possible, meaning to find the donor mass and radius, measurements of the white dwarf mass and radius, and the orbital separation between the two bodies. These metrics provide valuable probes of CV evolution \citep{knigge2006}, which is explored more thoroughly in \S\ref{sect:method:evolutionary modelling}.
This chapter describes in detail the two modelling techniques used for this thesis: the characterisation of a CV using multi-band eclipse modelling, and inferring the long-term mass loss rate of a system from its donor properties.
When fitting models to observations, the choice of algorithm is important. The majority of the parameter optimisation done in this work uses a type of Markhov Chain Monte Carlo (MCMC) technique, and this is also described in detail.


\section{Parameter optimisation}\label{sect:modelling:parameter optimisation}
Frequently in this work, a model must have its input parameters fit to observed or synthesised data. For models with few input parameters and well-behaved evaluation metrics, simple optimisation is possible, but often models have many parameters; for example the eclipse modelling portion of this work (\S\ref{sect:modelling:eclipse modelling}) has 18 parameters for a single eclipse, and fitting a full dataset frequently involves fitting 100 or more parameters. To make matters worse, the eclipse model is fairly expensive to compute in large numbers, making simple methods, like downhill simplex, untenable. Rather, we use MCMC.

The MCMC method is now a well-established tool in astronomy. It is robust, efficient, and yields the probability distribution of the variables being optimised even when not well-described by simple distributions. This has led to MCMC often being the method of choice when fitting models.
This section provides a working knowledge of MCMC, but for an in-depth introduction and review of the technique and its various sub-types see \citet{sharma2017}.


\subsection{Bayesian analysis}\label{sect:modelling:Bayesian analysis}
Bayesian inference uses known, `prior' knowledge combined with new information to derive a better understanding - the `posterior' knowledge of a model. This somewhat self-evident intuition is formalised Bayes' Theorem, which calculates the probability, $p(\theta | D, I)$ a set of model parameters, $\theta$, given some observed data, $D$, and background information, $I$.
\begin{equation}
    \label{eqn:modelling:bayes symbolic}
    p(\theta | D,I) = \frac{\mathcal{L}(D | \theta, I) \cdot q(\theta | I)}{p(D|I)}
\end{equation}
Here, $\mathcal{L}(D | \theta, I)$ is the probability of the observed data, given a model and prior information, so is called the likelihood of the data.
$q(\theta|I)$ is the probability of the model being valid, given some prior information, so is called the prior distribution. Finally, $p(D | I)$, or the probability of observing the data, given the previously known information, is also called the the `Evidence', and acts as a normalisation factor. Using this vocabulary, Equation~\ref{eqn:modelling:bayes symbolic} can be written as:
\begin{equation}
    \label{eqn:modelling:bayes english}
    \rm Posterior = \frac{Likelihood \times Prior}{Evidence}
\end{equation}
In Bayesian inference, the goal is to find the posterior distribution of the parameters of a model, given some data and, if possible, prior information.


\subsection{MCMC optimisation}\label{sect:modelling:affine invariant MCMC}
The MCMC technique is a class of tools developed to approximate the distributions in Equation~\ref{eqn:modelling:bayes symbolic}, without the need to process them analytically - a process that quickly becomes prohibitively difficult as the number of variables increases.

An MCMC sampler, as the name suggests, is a combination of a Monte Carlo method, a class of algorithms that rely on random sampling to find a result, and a Markov chain, a mathematical system that transitions between states according to probabilistic rules \citep{foreman2012}.
An MCMC randomly samples the prior distributions of the model variables (the Monte Carlo half of the algorithm), evaluates their $\mathcal{L}$ and $q$, and either accepts them onto its chain of sampled points or not, depending on if they meet a set of conditions. If the $\mathcal{L}$ of the proposed set of variables is higher than the $\mathcal{L}$ of the last set on the chain, the proposed set of variables is accepted. If $\mathcal{L}$ is lower, the algorithm randomly accepts or rejects the proposed step.
Conveniently, $\mathcal{L}$ is usually related to the $\chi^2$ metric by $\mathcal{L} \propto \mathrm{exp}(-\chi^2/2)$, so the change in $\mathcal{L}$ is often relatively easy to compute.
The method by which acceptance is decided is the sampling method, and many choices exist for different types of problems. The sampling method used here is the affine invariant sampling method with parallel tempering, described below.

By giving a finite chance to accept a `worse' set of parameters, the chain is, in theory, allowed to explore the possible parameter space without being trapped in local minima, given an infinitely long chain.
As the number of samples increases, the distribution of samples on the MCMC chain approaches the `true' distribution of the model variables, given a set of observations.


\subsubsection{Affine invariant ensemble sampling}\label{sect:modelling:Affine invariant ensemble sampling}
The affine invariant ensemble method of sampling was developed by \citet{goodman2010}, and makes use of many `walkers' sampling the parameter space in tandem.
Each walker functions as an individual MCMC chain, and the walkers interact by proposing steps based on the current states of other walkers. To propose a new parameter vector for walker $k$, the algorithm proposes a `stretch move'. Another walker, $j$ is chosen at random, and the last positions on each chain, $\theta_{j,N}$ and $\theta_{k,N}$ respectively, are used to propose a new position, $\Theta_k$, that lies somewhere on the line connecting the two position vectors.
\begin{equation}
    \Theta_k = \theta_j + z \cdot (\theta_k - \theta_j)
\end{equation}
The variable, $z$, determines the location on the line of the new vector, and is randomly drawn from a probability distribution $g(z)$,
\begin{equation}
    g(z) \propto
    \begin{cases}
        \frac{1}{\sqrt{z}}, & \frac{1}{2} \le z \le 2 \\
        0, & Otherwise
    \end{cases}
\end{equation}
A schematic of this stretch move concept is shown in Figure~\ref{fig:modelling:stretch move}.
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{figures/modelling/stretch_move.png}
    \caption{Reproduced from \citet{goodman2010}, showing the concept of a stretch move proposal.}\label{fig:modelling:stretch move}
\end{figure}

Then, $\Theta_k$ is either accepted or rejected from the chain depending on the current state of the chain. Recall that if the proposed $\mathcal{L}_{N+1} > \mathcal{L}_{N}$, i.e. the likelihood has improved, the sample is accepted. However, if $\mathcal{L}_{N+1} < \mathcal{L}_{N}$, the acceptance is determined by the transition probability, $P(\Theta_k | \theta_{k,N})$, defined as
\begin{equation}
    P(\Theta_k | \theta_{k,N}) = \alpha(\Theta_k | \theta_{k,N}) \cdot q(\Theta_k | \theta_{k,N})
\end{equation}
where $\alpha(\Theta_k | \theta_{k,N})$ is the acceptance probability,
\begin{equation}
    \alpha(\Theta_k | \theta_{k,N}) = \mathrm{min}\bigg( 1,\ z^{n-1}\frac{\mathcal{L}(\Theta_k)}{\mathcal{L}(\theta_{k,N})} \bigg)
\end{equation}
for a model with $n$ dimensions.
A random number, $u$, is drawn from a uniform distribution from 0 to 1, and if $u < \alpha(\Theta_k | \theta_{k,N})$, the proposed position is accepted.
In other words, the larger the drop in the probability that a new $\Theta_k$ describes observation, the \textit{less likely} the algorithm is to accept $\Theta_k$ onto the chain.
At each step in the MCMC, every walker has a new position proposed this way.

This finite chance to accept a new point, even if it is less likely to describe the data, makes the chain \textit{reversible} and allows the algorithm to explore the full allowable parameter space (given an infinite number of steps) even if doing so first requires moving to a less preferred $\theta$.
The affine invariant ensemble sampler benefits significantly from having the walkers in the ensemble interact, as they can communicate to other walkers regions of high $\mathcal{L}$ even between walkers in different local minima.
This improves the ensemble's ability to both locate the global minimum, and to sample non-spherical probability distributions (a task that is very difficult for simpler sampling techniques).
Further, this algorithm is able to have the proposal of new steps in every chain performed \textit{simultaneously}, significantly improving computation time; a typical rule-of-thumb is to use $2n$ walkers, and since the number of walkers is almost always larger than the number of available threads, increased evaluation time scales well with more threaded computation.


\subsubsection{Parallel tempering}\label{sect:modelling:parallel tempering}

In metallurgy, tempering is a way to increase the toughness of a metal by first heating it to a high temperature, then slowly cooling it, a process that relieves internal stresses.
While the metal is at a high temerature, carbon is able to diffuse throughout the crystal structure of the metal and explore possible crystalisation locations. As the metal slowly cools, carbon atoms are gradually more and more attracted to areas of the crystal that exert less stress on the material, until the metal is fully cooled and the majority of carbon atoms have found areas of local minima in stress potential.

The parallel tempered ensemble MCMC can take analogy from this `hot' exploration phase, and a `cool' settling phase with the walkers. This is done by running several parallel ensembles, that each have a different `temperature' between 1 and $\infty$. Each ensemble samples a modified posterior, that follows $\pi_T(\Theta)$;
\begin{equation}
    \pi_T(\Theta) = [\mathcal{L}(\Theta)^\frac{1}{T}] q(\Theta)
\end{equation}
At $T \rightarrow \infty$, the chain samples the prior with no respect to how well $\Theta$ describes the data. This `hot' chain is analagous the the diffusive carbon atoms, and is free to randomly explore available parameter space, potentially finding regions of high likelihood far from the initial conditions and communicating these regions back to cooler ensembles. Cooler temperatures between are analagous to the cooling metal -- drawn increasingly strongly towards regions of high likelihood.
The `cold' case of $T \equiv 1$ behaves exactly as a normal ensemble sampler.

Note that because hotter walkers are less sensitive to $\mathcal{L}$, the posterior no longer accurately reflects the `true' values. If a parameter is described by a Gaussian distribution with a standard deviation, $\sigma$, the tempered $\mathcal{L}$ will have a standard deviation of $\sigma \sqrt{T}$.

When running a parallel tempered MCMC, the values used in the software are $\beta = 1/T$, for which we choose between 3 and 5 evenly spaced values of $\beta$ between 0 and 1.
The number of temperatures used depended on the evaluation time. Since parallel tempering runs multiple full ensembles in tandem, for models with very large numbers of parameters it becomes highly desirable to use as few temperatures as possible. However, this penalty in computation time per step comes with the benefit of a dramatically improved ability to locate the global minimum, especially in complex parameter space.



\section{Finding an orbital ephemeris}\label{sect:modelling:getting ephemeris}

Crucial to both observing and modelling an eclipse is a good knowledge of the orbital ephemeris. This is described by the equation
\begin{equation}
    \label{eqn:modelling:general ephemeris equation}
    T_{\rm ecl} = T_0 + P_{\rm orb} E
\end{equation}
where $T_{\rm ecl}$ is the time of mid-eclipse, $T_0$ is the mid-eclipse time of the zeroth eclipse, and $E$ is the eclipse number. Accurately calculating $T_{\rm ecl}$ is important to scheduling observations of a system, and $P_{\rm orb}$ is a crucial to the eclipse modelling.

As observations are often separated by several months or even years, an error in $P_{\rm orb}$ of even $\sim 0.1$ seconds can accumulate to give significantly inaccurate predicted eclipse times. This need for precision also requires the definition of {\it where} a time is recorded from. All eclipse times presented in this thesis are given in the Barycentric Modified Julian Date (BMJD), which is the time of eclipse as measured from the centre of mass of the solar system. Note that this is different to the heliocentric MJD often seen in the literature, and where heliocentric literature values are used, they are first converted to BMJD.

\subsection{Finding eclipse times}\label{sect:modelling:finding eclipse times}

When finding an eclipse time, simply taking a time of minimum light is insufficient for the systems in this work. This is because it is common for CV eclipses to have very flat eclipse minima, and because CV eclipses have a fairly complex structure.
A somewhat more sophisticated method is necessary, and finding the mid-eclipse time is done by looking at the numerical derivative of an eclipse.
First, an eclipse is smoothed to remove short term fluctuations, partly those due to noise but also to mitigate the short term flickering often seen in CVs. This initial smoothing is done by applying a median filter to the data. Then, the numerical derivative is calculated and smoothed again, this time with a `boxcar' convolution. Properly filtered, the dominant remaining features of the numerical derivative are the ingresses and egresses of the white dwarf and bright spot.
The white dwarf ingress and egress are, in theory, symmetrical -- the ingress should be a sharp, negative spike, and egress should be a sharp, positive spike. As the two should be the same shape, a double-gaussian is fit to the derivative, using manually chosen initial conditions. In this model, the two gaussians share a width, $\sigma$, and have their mid-points described by
\begin{align*}
    T_{\rm 1,2} = T_{\rm ecl} \pm \Delta T \\
    h_{\rm 1,2} = \pm h
\end{align*}
where $T_{1,2}$ are the respective midpoints of the two gaussians, $h_{1,2}$ are their respective heights, and $2\Delta T$ is the distance between the two gaussians.
The derivative is then fit with these free parameters using an MCMC, to give the $T_{\rm ecl}$.

\subsection{Computing period}\label{sect:modelling:Computing ephemeris}

To find a rough initial ephemeris of a system, at least two eclipse observations with known $E$ are necessary. Given no prior knowledge of $P_{\rm orb}$ and $T_0$, this can be done by simply observing the system for several hours, until two consecutive eclipses are seen. This gives a rough measure of $P_{\rm orb}$, but can be significantly refined with longer baseline observations.
For each observed $T_\mathrm{ecl}$, $E$ could unambiguously be determined, either from observing consecutive eclipses or from previously reported literature values.
An MCMC algorithm was used to fit a straight line model to the independent variable $E$\ and dependent variable $T_\mathrm{ecl}$, with a gradient $P$\ and intercept $T_0$.
The model also accounts for potential systematic differences in timing accuracy between instruments by also having variable error scale factors applied to all eclipses observed with a specific instrument, e.g. the timing reported for eclipses observed with ULTRACAM may be systematically offset from reality, and the errors associated with those observations might need to be larger than reported to be consistent with data from other instruments. The prior distribution assumed for these error factors was log-uniform ranging from 0.01 to 100, which favours the smallest factor consistent with the data.

The value of $E$ for each eclipse were chosen to minimise the covariance between $T_0$ and $P$.
Consider a predicted eclipse time for $E$. The uncertainty on $T_{\rm ecl}$ in Equation~\ref{eqn:modelling:general ephemeris equation} can be written as,
\begin{equation}
    \sigma_{T{\rm ecl}}^2 = \sigma_{\rm T0}^2 + 2\sigma_{\rm T0} \sigma_{\rm P}E + \sigma_{\rm P}^2 E^2
\end{equation}
from the standard error propagation formula.
To evaluate an alternative set of $E$, $E$ can be offset by some integer, $N$, with $E' = E - N$. Then, by expanding out the brackets and consolidating some terms, $\sigma_{T{\rm ecl}}^2$ becomes
\begin{align*}
    \sigma_{T{\rm ecl}}^2 =& \sigma_{\rm T0}^2 + 2\sigma_{\rm T0} \sigma_{\rm P}(E'+N) + \sigma_{\rm P}^2 (E'+N)^2 \\
    =& (\sigma_{\rm T0}^2 + \sigma_{\rm P}^2 N^2 + 2\sigma_{\rm T0} \sigma_{\rm P} N) + 2(\sigma_{\rm T0} \sigma_{\rm P} + \sigma_{\rm P}^2 N)E' + \sigma_{\rm P}^2 E'^2
\end{align*}
Since we want to minimise the $\sigma_{T{\rm ecl}}^2$ term, we can choose a value of $N$ to do so, $N = -(\sigma_{\rm T0}\sigma_{\rm P})/(\sigma_{\rm P}^2)$ rounded to the nearest integer.


\section{Characterising a CV}\label{sect:method:lightcurve modelling}

To determine the system parameters for the three CVs in this study, the eclipse lightcurves were modelled. This method is more frequently applicable in CVs than the more traditional approach of using spectroscopic eclipsing binaries, since the donor star is rarely directly visible. Compared to using the superhump period excess to estimate the mass ratio \citep{patterson2005, knigge2006}, lightcurve modelling requires few assumptions. However, it does require precise alignment of the system and so is not possible for a large fraction of CVs.

CV eclipse modelling was first developed by \citet{wood1986}, and has been refined significantly over the last decade \citep{Savoury2011, McAllister2017, McAllister2019}. This method relies of four assumptions: \textit{(1)} the stream of mass flowing from the donor to the white dwarf follows a ballistic trajectory, \textit{(2)} the white dwarf obeys a theoretical mass-radius relationship, \textit{(3)} the white dwarf is unobscured by the accretion disc or other sources of intra-system material, and \textit{(4)} the donor exactly fills its Roche lobe. Most of these assumptions are considered robust, though the visibility of the white dwarf been called into question by \citet{Spark2015}.

Assuming that the secondary star completely fills its Roche lobe (which is required for mass transfer) ensures that the donor radius is solely a function of $q$. The white dwarf eclipse width is set by the width of the donor, the separation between the two stars, and $q$. The width of the white dwarf eclipse, therefore, is solely a function of $q$, $P_{\rm orb}$, and inclination, $i$ \citep{bailey1979}.

Assuming that the mass stream between the two stars follows a ballistic trajectory that follows a path determined by $q$ \citep{Lubow1975}, the location of the bright spot can be fixed in space as the point at which the ballistic stream intersect the outer edge of the accretion disc. Therefore, the phase of the bright spot ingress and egress is a function of $q$, and the disc radius.

Assuming that the white dwarf is unobscured allows us to directly relate the inclination, and the duration of ingress and egress to the white dwarf's radius. Because the white dwarf is assumed to follow a known mass-radius relationship, by fitting the observed white dwarf colours with a temperature, gravity, distance and interstellar extinction, the temperature and radius of the white dwarf yield a mass. The donor mass is then a simple product of the white dwarf mass, and $q$.

The final result of modelling is the following parameters,
\begin{itemize}
    \item white dwarf and donor masses
    \item white dwarf and donor radii
    \item orbital separation
    \item orbital velocity of the white dwarf and donor
    \item inclination
    \item white dwarf effective temperature and surface gravity
    \item distance
\end{itemize}

The modelling actually takes place in two phases, which are each described in detail - first the phase-folded eclipse is modelled, then the resulting proxy variables are converted to physical parameters. Finding the best-fitting parameters to observed data is complex, and is described in \S\ref{sect:modelling:optimising eclipse model parameters} \todo{Put in some history here -- old versions using the derivative, comparison with other methods, past analysis with this method.}

\subsection{Phase-folded eclipse modelling}\label{sect:modelling:eclipse modelling}

Recall that the light from a CV originates from four distinct objects in the system. The white dwarf and donor star, the accretion disc about the white dwarf, and the bright spot impact region (hereafter simply `the bright spot'), where transferred material impacts the outer rim of the accretion disc and liberates significant amounts of energy. Notably the bright spot emits flux directionally, so
The anatomy of a CV eclipse lightcurve is a sequence of five events, that occur in the following order:
\begin{enumerate}
    \item a pre-eclipse hump is often seen as the bright spot rotates into view
    \item The white dwarf becomes obscured by the donor
    \item The bright spot becomes obscured by the donor
    \item The white dwarf emerges from behind the donor
    \item The bright spot emerges from behind the donor
\end{enumerate}
Figure\todo{Make a figure for this} shows a typical lightcurve, with these events marked.

The model schematic of a CV in \lstinline{lfit} is shown in Figure\todo{make a schematic for this -- labelled!}. At this stage, parameters are not yet in SI units, which is done to make the modelling software more efficient. Rather, the phase-folded lightcurve is generated, causing parameters to be scaled, i.e. $q$ is fit rather than masses directly, and distances are scaled to the distance from the white dwarf primary to the $L_1$ point, $x_{l1}$
A single eclipse is described by 18 parameters:
\begin{itemize}
    \item White dwarf, donor star, disc, and bright spot fluxes, $F_{\rm wd,\ donor,\ disc, bs}$
    \item Mass ratio, $q$
    \item White dwarf eclipse width in units of phase, $\Delta \phi$
    \item Scaled white dwarf radius, $R_{\rm wd} / x_{l1}$
    \item White dwarf limb darkening coefficient, $u_{\rm ld}$
    \item Scaled outer disc radius, $R_{\rm disc} / x_{l1}$
    \item Disc surface profile exponent, $b$
    \item Seven parameters describing the bright spot
    \item An eclipse phase offset, $\phi_0$
\end{itemize}
The seven bright spot parameters are not physically motivated, but relate to the bright spot model.

\subsubsection{The white dwarf}

The white dwarf is modelled as a disc, with a total surface brightness $F_{\rm wd}$ and a radius of $R_{\rm wd} / x_{l1}$. It is subject to limb darkening, using a linear prescription:
\begin{equation}
    \frac{I_l}{I_0} = 1 - u_{\rm ld}(1 - {\rm cos}\beta)
\end{equation}
where $I_0$ is the intensity at the centre of the disc, and $I_l$ is the intensity at a limb. $\beta$ is the angle between the line normal to the surface of the white dwarf, and the observer's line of sight.

\subsubsection{The donor star}

The secondary star does not become obscured during an eclipse, but there is some variation in its brightness. The donor is not spherical, so a small ellipsoidal variation is seen as it rotates to expose more or less of its surface to the observer.

\subsubsection{The accretion disc}

The accretion disc is modelled as a series of annular rings about the white dwarf, extending out to $R_{\rm disc} / x_{l1}$. The intensity of each annulus decreases with distance from the white dwarf, following an exponential formula, $I_i \propto R^{-b}$ for ring $i$ at distance $R$ from the white dwarf. As $b$ is a free parameter in the model, discs can be made more or less centrally concentrated to match observations. As the bright spot location is determined by $q$ and $R_{\rm disc} / x_{l1}$, the phases of bright spot ingress and egress provide a valuable diagnostic for $R_{\rm disc} / x_{l1}$.

\subsubsection{The bright spot}

The bright spot model is not physically motivated, but rather is chosen to be able to reproduce a large range of bright spot eclipses.
It is modelled as a strip of flux extending in both directions tangentially from the edge of the disc, with a defined brightness profile and overall flux. The strip intensity falls off exponentially, described by the equation
\begin{equation}
    I_{\rm X} \propto \bigg ( \frac{X}{S} \bigg )^Y \cdot {\rm exp} \bigg [ - \bigg (\frac{X}{S} \bigg )^Z \bigg]
\end{equation}
where $I_{\rm X}$ is the intensity of the strip a distance $X$ along it, and $S$ is the scale of the bright spot. $Y$ and $Z$ are the profile exponents.

The bright spot is known to emit light directionally, at a beaming angle, $\theta_{\rm yaw}$, from the normal to the strip in the plane of the disc, and an angle $\theta_{\rm tilt}$ from the plane of the disc.
Some fraction of the light is beamed, and the rest, $f_{\rm is}$, is emitted isotropically from the strip. This geometry is shown in Figure~\ref{fig:modelling:bright spot schematic}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/modelling/bright_spot_schematic.pdf}
    \caption{Showing a schematic of the bright spot model. The lower dashed line joins the centres of the white dwarf and donor stars, and the upper dashed line runs parallel to it, intersecting the bright spot location. The thick red line is the flux-emitting strip, and the arrow shows the direction of light emission, at an angle $\theta_{\rm yaw}$ from the normal.}
    \label{fig:modelling:bright spot schematic}
\end{figure}

Lower values of $q$ will cause the ballistic stream to take a wider arc towards the white dwarf, moving the intersection point with the disc. The location of the bright spot on the disc edge is defined by $\theta_{\rm az}$, the angle between the strip and the line of sight of the observer.

As the bright spot is the most complex component of the model, there is an option to simplify it in software for systems with faint bright spot features that cannot be properly resolved.
This mode is called the `simple' bright spot model, and fixes $\theta_{\rm tilt}$ at $90\deg$, $\theta_{\rm yaw}$ at $0\deg$, and the strip exponents $X$ and $Y$ to 1 and 2, respectively. By removing these four degrees of freedom, better characterisation of the eclipse is possible.

\subsection{Post-processing the eclipse model}\label{sect:modelling:post processing the eclipse model}

The eclipse modelling uses proxy variables, so some processing must be done to convert them to `real' values. This is done in two steps. First, the white dwarf colours are fit to find a white dwarf temperature and surface gravity. Then, the white dwarf temperature and orbital period are combined with the best-fit eclipse model parameters to convert the scaled distances to metres, and mass ratio to kilograms.

\subsubsection{Fitting white dwarf colours}\label{sect:modelling:fitting white dwarf colours}
By modelling the eclipse in multiple bands, at least three observations of white dwarf flux are available. These fluxes are fit to the DA white dwarf cooling model from \citet{Bergeron1995}\footnote{Available from \href{http://www.astro.umontreal.ca/~bergeron/CoolingModels}{http://www.astro.umontreal.ca/~bergeron/CoolingModels}}, from which the absolute magnitude of the white dwarf in each band, $M$, can be retrieved from pre-calculated cooling tracks for a given effective temperature, $T_{\rm eff}$ and surface gravity, $\log (g)$. This absolute magnitude is then easily translated to an apparent magnitude, $m$, given a system parallax, $\pi$, and interstellar extinction coefficient, $\rm E(B-V)$,
\begin{equation}
    m = M - 5\log (\pi\mathrm{,\ arcsec}) - 5
\end{equation}

To optimise these four parameters, an affine-invariant MCMC with three levels of parallel tempering was used, c.f. \S\ref{sect:modelling:ensemble MCMC}. For the priors, uniform $T_{\rm eff}$ and $\log (g)$ distributions were used that span the range set by the model cooling tracks. $\rm E(B-V)$ used a uniform distribution between 0, and the maximum IRSA measurement for the relevant coordinates\footnote{Available \href{https://irsa.ipac.caltech.edu/applications/DUST/}{https://irsa.ipac.caltech.edu/applications/DUST/}}, and the parallax prior was chosen to match the Gaia measurement \citep{lindegren2018, Luri2018, Gaia2016, Gaia2018} of the system.

\subsubsection{Conversion of proxy variables to physical parameters}
\label{sect:modelling:conversion to physical parameters}
The eclipse model proxy variables are then converted to real values. This is done using the white dwarf as a known quantity, based on white dwarf model atmospheres. Five input variables are needed: $T_{\rm eff}$, $P_{\rm orb}$, $q$, $\Delta \phi$, and $R_{\rm wd} / x_{l1}$.


A measure of the white dwarf radius, $R_{\rm wd}$, can be found using Kepler's 3rd law and making the substitutions $r = R_{\rm wd} / a$ and $q = M_{\rm wd} / M_{\rm donor}$.
\begin{align}
    P_{\rm orb}^2 &= \frac{4 \pi^2 a^3}{G (M_{\rm wd} + M_{\rm donor})} \\[8pt]
    &= \frac{4 \pi^2 R_{\rm wd}^3}{G M_{\rm wd} (1 + q) r^3} \\[8pt]
    R_{\rm wd}^3 &= \frac{P_{\rm orb}^2 r^3 G M_{\rm wd} (1 + q) }{4 \pi^2}
    \label{eqn:modelling:geometric radius}
\end{align}
$r$ can be found from $R_{\rm wd} / x_{l1}$ by calculating $x_{l1} / a$, which itself is a function only of $q$.

This value of $R_{\rm wd}$ is geometric mass-radius relationship, and requires the white dwarf mass. Fortunately, for a given $T_{\rm eff}$ (which is known from the colour fits, \S\ref{sect:modelling:fitting white dwarf colours}), white dwarfs follow tight theoretical mass-radius relationships, that can be employed to find the unique $M_{\rm wd},\ R_{\rm wd}$ pair that satisfies both Equation~\ref{eqn:modelling:geometric radius} and the theoretical mass-radius relationship.
Specifically, a proposed theoretical mass-radius pair is chosen from a model relationship and a value of $R_{\rm wd, calc}$ is calculated from Equation~\ref{eqn:modelling:geometric radius}. If this matches the theoretical value, the mass is valid. Otherwise, the proposed mass is altered accordingly and a new mass-radius pair is checked.

Three white dwarf mass-radius relations were used. First, a solution was searched for using the \citet{wood1995} models, spanning masses of $0.4 - 1.0 M_\odot$.
If no solution could be found, the \citet{panei2000} models were searched, spanning masses from $0.4 - 1.2 M_\odot$.
Both of these mass-radius relationships account for the white dwarf $T_{\rm eff}$. However, if no solution has been found, the \citet{hamada1961} 0 Kelvin mass-radius relation is checked for solutions. This track spans the largest range in mass, from $0.14 - 1.44 M_\odot$. If no solution is found with the \citet{hamada1961} tracks, the model is considered invalid, though this did not occur for any system in this thesis.

Then, the inclination is calculated. $\Delta \phi$ is solely a function of $q$, and $i$. Therefore, we can use the known values of $\Delta\phi$ and $q$ to calculate the system inclination - this is done by proposing candidate values of $i$, and comparing the calculated $\Delta \phi_{\rm calc}$ with the target $\Delta \phi$, and adjusting as needed until the two agree.

Now, three quantities are known; $i$, $M_{\rm wd}$, and $R_{\rm wd}$. As previously mentioned, $R_{\rm donor}$ is assumed to be the Roche radius, from Equation~\ref{eqn:introduction:eggleton approximation}, and $M_{\rm donor}$ is found simply by $(q \cdot M_{\rm wd})$. Finally, the orbital velocities, $K_{\rm wd,\ donor}$ respectively, of the two stars are calculated using Kepler's laws.
\begin{align}
    K_{\rm wd} &= \frac{2\pi a \mathrm{sin}i}{P_{\rm orb}} \frac{q}{1+q} \\
    K_{\rm donor} &= K_{\rm wd} \cdot \frac{M_{\rm wd}}{R_{\rm wd}}
\end{align}


\subsection{Capturing flickering with Gaussian Processes}
Explain GPs from scratch, and get into detail about which kernels you're using.


\subsection{Hierarchical model structure}\label{sect:modelling:optimising eclipse model parameters}

I extend the lightcurve fitting model used by \citet{McAllister2019}, adopting a hierarchical approach to slightly reduce model complexity.

Changes in the disc radius and brightness profile, and bright spot parameters can mean that the same CV has a significantly different eclipse lightcurve at different times, making it difficult to justify averaging together many eclipses, as features can become smeared out and uninformative. In the worst-case scenario, all 18 parameters would be independently variable for each eclipse, in each band. However, by sharing some parameters between eclipses and bands, this large number of free parameters is slightly reduced, and the posterior of some parameters can be informed by multiple eclipses. \citet{McAllister2017} share $q,\ R_{\rm wd} / x_{l1}$, and $\Delta\phi$ between eclipses, and we extend that concept by organising the model into a hierarchical tree structure, a schematic of which is shown in Figure~\ref{fig:method:hierarchical_model}.

\begin{figure}
    \centering
    \includegraphics[width=.85\columnwidth ]{figures/three_cvs_with_weird_colours/GeneralFigs/hierarchical_model_structure.png}
    \caption{The hierarchical structure of the lightcurve model. Parameters are inherited downwards, to produce an eclipse at the `leaves' of the tree, e.g. Eclipse 3 inherits the parameters of Band 2, which in turn inherits the Core parameters. $\mathrm{F_{WD, RS}}$\ represent the fluxes of the white dwarf and donor star, and $\mathrm{U_{LD}}$\ is the limb darkening coefficient of the white dwarf.}
    \label{fig:method:hierarchical_model}
\end{figure}

The top level of the model provides the core parameters, which are unchanging between all observing bands and constant across our observations: $q,\ R_\mathrm{WD}/a$, and $\Delta\phi$. We assume the white dwarf and donor fluxes do not change on the timescale of our observations, and so these variables, along with the limb darkening coefficient of the white dwarf, are shared between all eclipses observed with the same filters. The bottom level holds parameters that can vary quickly enough to change between eclipses, i.e. parameters describing the accretion disc and bright spot. By handling parameters this way, we maximise the amount of data informing important variables, for example, white dwarf fluxes and $q$. We also somewhat reduce the number of free parameters, which aids slightly in model fitting, but the chief justification for the hierarchical approach is that it ensures consistency between eclipses - something not guaranteed when fitting eclipses individually.

As more eclipses are added, the number of dimensions in parameter space that must be explored increases. For illustration, the model for ASASSN-17jf has 3 eclipses across 3 bands, plus 3 Gaussian process parameters, resulting in 87 free parameters that must be optimised simultaneously. To find the most likely set of lightcurve parameters in this very large space, an ensemble MCMC fitting code was used. The MCMC uses the \texttt{emcee} implementation of an ensemble sampler and parallel tempering \citep{foreman2012}, c.f. \S\ref{sect:modelling:affine invariant MCMC}.



\section{Evolutionary modelling}\label{sect:method:evolutionary modelling}

Once armed with a robust, somewhat sizable sample of characterised CVs, evolutionary modelling is able to refine our understanding even further. In \S\ref{sect:introduction:Summary of how AML and Mdot drive period evolution}, I motivated how the donor star inflates in response to mass loss, and how the degree of this inflation is related to the severity of the mass loss.
If the radius of an equivalent star in the absence of mass loss is known, and the observed radius of a CV donor reproduced by stellar structure models by the introduction of some amount of mass loss, the long-term average mass loss rate can be inferred.

The stellar evolution code used is the MESA codebase, and in order to be confident in any mass loss rates derived this way, two key demonstrations of model accuracy are necessary. However, there is cause for confidence; MESA is capable of reproducing the canonical \citep{knigge11} donor tracks with impressive accuracy, even reproducing the period gap by a shutdown of magnetic braking \citep{Paxton_2015} with very little adjustment from default parameters.
First, I demonstrate that MESA can reproduce the two \citet{knigge11} donor tracks -- recall from \S\ref{sect:introduction:the missing aml problem} that two tracks are constructed, a `standard' track with only typical gravitational braking below the period gap, and an `optimal' track that amplifies gravitational braking by $2.47\times$.
By default, MESA shuts off magnetic braking when the donor becomes fully convective, a practice which I motivate in \S\ref{sect:introduction:the missing aml problem} to be spurious. Instead, MESA is altered for a fixed magnetic braking cutoff at $0.2 M_\odot$, effectively arbitrarily fixing the donor mass of the period gap.
In addition, \citet{Pala2017a} added a subroutine to MESA that allows for the amplification of gravitational braking below the period gap. This was used to reproduce the `optimal' track. Figure~\ref{fig:modelling:MESA can reproduce the K11 tracks} shows how well this augmented MESA model can reproduce the \citet{knigge11} tracks.
\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{figures/modelling/compare_K11_withspot_normb_and_1xrmb_fixedcutoff.png}
    \caption{Showing how well MESA can reproduce the canonical \citet{knigge11} donor tracks. {\bf Solid lines} are MESA tracks, and {\bf dotted lines} are the \citet{knigge11} tracks. {\bf Black} lines have only gravitational braking below the period gap, and {\bf red} lines gave gravitational braking at $2.47\times$ strength.}
    \label{fig:modelling:MESA can reproduce the K11 tracks}
\end{figure}

To extract mass loss rates, I will go through three steps.
Firstly, I demonstrate how the radius of a singleton model of a given mass can be tuned to match observations by introducing star spots, and produce a set of M dwarf models that reproduce observations of stars across the CV donor mass range that \textit{aren't} losing mass.
Secondly, I explore the range of masses for which the radius is a reasonable diagnostic for mass loss rate.
Then, I outline the method by which I search for mass loss rates that produce stellar models matching CV donor observations.
Note that all code used for these steps is publicly available online.\todo{But not until Meridith says so!!!!!!}


\subsection{For what range of masses can we extract mass loss rates?}
\label{sect:modelling:MESA massloss allowable mass range}

It is worth evaluating the feasibility of using the radius to extract mass loss rates. Recall from \S\ref{sect:introduction:period minimum and bouncers} the two timescales that govern the response of the donor to mass loss: $\tau_{\rm KH}$ and $\tau_{\dot M}$. These timescales are calculated by
\begin{align}
    \tau_{\rm KH} \sim& \frac{G M_{\rm donor}^2}{L_{\rm donor} R_{\rm donor}} \\[8pt]
    \tau_{\dot M} \sim& \frac{\dot M}{M_{\rm donor}}
\end{align}

While $\tau_{\rm KH} \ll \tau_{\dot M}$, the donor is able to maintain thermal equilibrium and is indistinguishable from a singleton star of the same mass.
If $\tau_{\rm KH} \gg \tau_{\dot M}$, the donor is \textit{not} able to maintain equilibrium; mass loss is fast and adiabatic. The donor is inflated by mass loss, but because the star cannot adjust on thermal timescales, the degree of inflation is dependent on the mass loss \textit{history} of the donor.
However, calculating the two timescales for CVs reveals that $\tau_{\rm KH} \sim \tau_{\dot M}$ \citep{knigge11} - meaning that most CV donors are \textit{almost} able to maintain thermal equilibrium, but are still mildly affected by mass loss.
Under this almost-equilibrium regime, mass loss induces some degree of radius inflation in the donor, but because the star is still almost able to adjust on thermal timescales, the degree of inflation does \textit{not} depend on the mass loss history. In this regime, we can discard the mass loss history of the donor, and use the radius inflation as a diagnostic for the long term baseline mass loss rate.

Which regime the donor falls into is a function of $M_{\rm donor}$. As $M_{\rm donor}$ falls, $\tau_{\rm KH}$ begins to rise faster than $\tau_{\dot M}$; Figure~\ref{fig:modelling:how does tauKH and tauMdot vary with donor mass} shows this trend, produced by a MESA model of a CV c.f. \citep{Paxton_2015,Pala2017a}.
The rise in $\tau_{\rm KH}$ relative to $\tau_{\dot M}$ begins to become significant at $\sim 0.1 M_\odot$, around the mass the donor enters the fast, adiabatic $\tau_{\rm KH} \gg \tau_{\dot M}$ period bouncer phase c.f. \S\ref{sect:introduction:period minimum and bouncers}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/tau_both_vs_donor_mass_AML000.pdf}
    \caption{Showing how the two timescales, $\tau_{\rm KH}$ and $\tau_{\dot M}$ vary with donor mass below the period gap in CV donors, as modelled by MESA \citep{Paxton_2015,Pala2017a}.}
    \label{fig:modelling:how does tauKH and tauMdot vary with donor mass}
\end{figure}

It would be better to know exactly how much error is introduced by the donor moving out of the $\tau_{\rm KH} \sim \tau_{\dot M}$ regime to draw a more informed mass limit, and this too can be extracted from modelling.
First, I ran a series of donor-like singleton models with varying degrees of mass loss rates, uniformly spaced between $\log (\dot M, M_\odot \mathrm{yr}^-1) = -9.5 \rightarrow -11$.
Then, a series of MESA CV models were run with gravitational losses amplified by $x = 1 \rightarrow 6$.
Then, each model has its radius, $R$, and $\dot M$ extracted at $0.1 M_\odot$. Since the CV models have varying $M_\odot$ and the singleton models do not, if $\dot M$ history does not affect radius inflation, the radii between the two sets of models will match. Figure~\ref{fig:modelling:comparing radii at 0.1Msun} shows this, and little divergence between the two sets of radii is visible.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/compare_0.1Msun_with_CV_track_K11_fig1.pdf}
    \caption{Showing the radius and mass loss extracted from MESA models at $0.1 M_\odot$. The {\bf black} line is a series of singleton models with constant mass loss, and the {\bf red} line is a series of CV models with gravitational AML amplified by $x = 1 \rightarrow 6$.}
    \label{fig:modelling:comparing radii at 0.1Msun}
\end{figure}

By plotting the difference between the two sets of models, and repeating the same process for a range of masses, Figure~\ref{fig:modelling:comparing radii over a range of masses} is produced. Now, by looking at what level of divergence historical changes in $\dot M$ induces, we can evaluate what mass range is acceptable. The upper limit on mass must be $0.2 M_\odot$, as this is the enforced mass of the period gap, and for a lower limit I impose an acceptable level of disagreement of $3\%$. It can be seen that the minimum acceptable mass is then $0.08 M_\odot$.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/compare_multiple_mass_with_CV_K11_fig1a.pdf}
    \caption{The inflation of CV model radii, $R_{CV}$, over singleton model radii, $R_S$, from Figure~\ref{fig:modelling:comparing radii at 0.1Msun}, for a range of masses.}
    \label{fig:modelling:comparing radii over a range of masses}
\end{figure}


\subsection{Modelling star spots in MESA}\label{sect:modelling:starspots in MESA}

A general problem in the modelling of low mass stars is an under-estimation of their radii, and MESA is no exception. Recently, Brown et. al. (2022)\todo{Get this reference. Can't be long now!} characterised a sample of $\sim 15000$ M dwarfs in detached binaries from \citet{parsons2018} with a 5th order polynomial, referred to as the Brown relation. We use this empirical mass-radius relation as the baseline `zero mass loss' benchmark radius for comparison against models.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/MESA_inflation_over_brown.pdf}
    \caption{Showing the radius inflation of MESA models over the Brown relation, i.e. $(R_{\rm MESA} - R_{\rm brown}) / R_{\rm brown}$. The horizontal dashed line is the target radius inflation for MESA models.}
    \label{fig:modelling:MESA inflation over Brown relation}
\end{figure}
The difference between singleton MESA models with no mass loss and the Brown relation is shown in Figure~\ref{fig:modelling:MESA inflation over Brown relation}, however the goal is not to reproduce the Brown relation exactly.
CV donors are filling their Roche lobes, so are non-spherical. To account for this, \citet{knigge11} introduces a $4.5\%$ radius inflation over isolated stars, and I mirror this approach. Thus, the target radius inflation in Figure~\ref{fig:modelling:MESA inflation over Brown relation} is $4.5\%$.

To achieve this inflation, I introduce star spots into MESA. Star spots are magnetic phenomena, where the magnetic pressure from concentrations of magnetic field lines provides partial pressure support to the photospheric material, and since spots must remain in pressure equilibrium with the spotted surface, the temperature in a spotted region is reduced by ideal gas laws. As a consequence, the cooler spotted regions emit less black body radiation, inhibiting energy flux out of the stellar interior, and inflating the star.

As MESA is a one-dimensional code and star spots are a two-dimensional phenomenon, spots are implemented with the model formulation given by \citet{sommers2015}, based on work by \citet{spruit1986}.
Under this spot treatment, two effects are implemented: the photosphere is made inhomogeneous by the presence of spots, and convection is inhibited by the presence of a strong magnetic field.
In the \citet{sommers2015} formulation, the former effect is enforced by altering the effective temperature to a surface-weighted average of the spotted and unspotted surface, and the latter by augmenting the radiative gradient of the star.

The spots have a coverage fraction of $f_{\rm spot}$, and a temperature contrast of $x_{\rm spot} = T_{\rm  spot} / T_{\rm amb}$, where the effective temperature of the spotted surface is $T_{\rm spot}$, and the effective temperature of the ambient, unspotted surface is $T_{\rm amb}$. The surface-weighted average, $T_{\rm av}$ is then,
\begin{equation}
    \label{eqn:modelling:surface weighted average temp}
    T_{\rm av}^4 = (1-f) T_{\rm amb}^4 + f_{\rm spot} T_{\rm spot}^4
\end{equation}
And the altered luminosity, $L_{\rm av}$, becomes
\begin{align}
    L_{\rm av} =& 4\pi R^2 \sigma_{boltz} T_{\rm amb}^4 (1 - f_{\rm spot} + f_{\rm spot} \cdot x_{\rm spot}^4) \\
    L_{\rm av} =& 4\pi R^2 \sigma_{boltz} T_{\rm amb}^4 \alpha_{\rm spot} \\
    L_{\rm av} =& L_{\rm amb} \alpha_{\rm spot} \\
\end{align}
$\alpha_{\rm spot}$, the redistribution parameter, is thus what actually alters the structure of the star, and is analogous to the blocking area of totally black spots, or spots that are completely supported by magnetic pressure.

MESA performs a lookup for the surface pressure from $T_{\rm eff}$ using precalculated boundary condition tables (see \citet{paxton2010,paxton2011} for details). I modify the MESA code to perform this lookup with $T_{\rm av}$.

However, in MESA $T_{\rm eff}$ is not used in the stellar model interior - rather, it uses energies and pressures to calculate structure. Therefore, I alter the above equation to use pressure instead.
Recall the ideal gas equation, for gas in the ambient, unspotted surface. This gas will have pressure and temperature $P_{\rm gas, amb}, T_{\rm gas, amb}$, density $\rho$, and mean molecular weight $\mu$, and the gas constant $R$.
\begin{equation}
    P_{\rm gas, amb} = \frac{\rho R}{\mu} T_{\rm gas, amb}
\end{equation}
The spotted and unspotted surfaces are under pressure (and therefore also density) equilibrium, but the spotted surface pressure has a contribution from gas pressure, $P_{\rm gas, spot}$, and some contribution from magnetic pressure, $P_{\rm mag, spot}$. Therefore, we can write\todo{This proof is a bit extra if I'm honest with myself, but I think I'd rather be clear?}
\begin{align}
    P = P_{\rm gas, amb} =& P_{\rm gas, spot} + P_{\rm mag, spot} \\
    \frac{\rho R}{\mu} T_{\rm amb} =& \frac{\rho R}{\mu} T_{\rm spot} + P_{\rm mag, spot} \\[12pt]
    P_{\rm mag, spot} =& \frac{\rho R}{\mu} (T_{\rm amb} - T_{\rm spot}) \\
    % P_{\rm mag, spot} =& \frac{\rho R}{\mu} (1 - x_{\rm spot}) T_{\rm amb} \\
    P_{\rm mag, spot} =& (1-x_{\rm spot}) P_{\rm gas, amb} \\[12pt]
    P_{\rm gas, spot} =& P_{\rm gas, amb} - P_{\rm mag, spot}\\
    % P_{\rm gas, spot} =& P_{\rm gas, amb} - (1-x_{\rm spot}) P_{\rm gas, amb} \\
    P_{\rm gas, spot} =& x_{\rm spot} P_{\rm gas, amb}
\end{align}
However, star spots do not penetrate to the core of the star. To quantify this, rather than fixing $x_{\rm spot}$ and calculating the new pressure at each depth of the star, I calculate the pressure differential at the surface of the star, and fix this pressure difference for interior layers. As the pressure rises with depth, a significant pressure difference at the surface quickly becomes insignificant. For each depth of the star, $x_{\rm spot}$ is calculated from its total pressure before any alteration is made, $P_i$, and the surface pressure, $P_{\rm surf, amb}$,
\begin{align}
    P_{\rm gas, amb} - P_{\rm mag, spot} =& x_{\rm spot} P_{\rm gas, amb} \\
    x_{\rm spot, i} =& \frac{P_i - P_{\rm surf, amb}}{P_i}
\end{align}
Rather than directly altering the pressure profile of the star, the radiative gradient, $\nabla_r$, at each depth is modified. MESA then uses $\nabla_r$ to compute a self-consistent pressure profile for the star.
\begin{align}
    \alpha_{\rm spot, i} =& 1 - f_{\rm spot} + f_{\rm spot} \cdot x_{\rm spot, i}^4 \\
    \nabla_{r,i}' =& \frac{\nabla_{r,i}}{\alpha_{\rm spot, i}}
\end{align}
Values of $f_{\rm spot}$ and $x_{\rm spot}$ can be passed to MESA as user-configured parameters. Figure~\ref{fig:modelling:spotted model radii at 2Gyrs} shows the radii of main sequence stellar models at 2~Gyrs, and $0.15 M_\odot$ with progressively more spots.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/spotted_model_radii_at_2gyrs.pdf}
    \caption{Showing how model radius varies as a function of spot coverage. Here, spots are perfectly black ($x_{\rm spot} \equiv 0$), and the radius is extracted at 2~Gyrs.}
    \label{fig:modelling:spotted model radii at 2Gyrs}
\end{figure}


\subsection{Tuning star spot parameters to observations}\label{sect:modelling:tuning star spots to observations}
With star spots implemented in MESA, the Brown relation can now be reproduced.
$x_{\rm spot}$ is fixed at 0 and $f_{\rm spot}$ is varied.
Since radius increases monotonically with $f_{\rm spot}$, a binary chop is performed, optimising for $\Delta R = R_{\rm MESA} - R_{\rm Brown} = 0$ at a stellar age of 2~Gyrs.

To find the appropriate $f_{\rm spot}$ for a given mass, the following steps are followed.
First, MESA models using the upper and lower bounds of $f_{\rm spot}$ ($f_{\rm up}$ and $f_{\rm low}$ respectively) are evaluated, and $\Delta R$ calculated for each.
The lower bound will have $\Delta R < 0$, and the upper bound will have $\Delta R > 0$, and since $\Delta R$ is a smooth function of $f_{\rm spot}$, $\Delta R \equiv 0$ must lie between them.
Then, $\Delta R$ at the midpoint, $f_{\rm mid} = (f_{\rm up} + f_{\rm low})/2$, is evaluated, and accepted as either the new $f_{\rm up}$ (if $\Delta R$ is positive) or $f_{\rm low}$ (if $\Delta R$ is negative).
A new $f_{\rm mid}$ is then calculated from the new range, and the cycle repeats until $\Delta R$ is within tolerance; set to $0.1\%$.
The resulting M-$f_{\rm spot}$ relation is shown in Figure~\ref{fig:modelling:fspot mass relationship}\todo{This figure matches the wrong inflation over observations!!!! running a new grid, replace this figure.}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/modelling/fspot_relation_to_match_brown_plus_3.5.pdf}
    \caption{The required $f_{\rm spot}$ that is applied to tune M dwarf MESA models to match the Brown relation, plus an added $4.5\%$ inflation due to non-spherical Roche geometry.}
    \label{fig:modelling:fspot mass relationship}
\end{figure}

Below masses of $\sim 0.13 M_\odot$, the required $f_{\rm spot}$ becomes negative, i.e. default MESA models are larger than observations. As this is unphysical, negative values of $f_{\rm spot}$ are set equal to 0, and we note that derived mass loss rates become unreliable below this mass.

There is significant scatter in the Brown mass-radius relation, that is not captured in these models. The inherent scatter in radius for the observations is \todo{GET THIS} between 0.1 and 0.2 $M_\odot$, which...
However, while this may skew an individual system, on average the inferred mass loss from model radius should be accurate. Therefore, I ignore this effect. \todo{wow. This sounds awful. write it better.}


\subsection{Optimising mass loss rate to donor observations}
\label{sect:modelling:optimising mass loss rate to observations}

Finally, the mass loss rate required to match donor observations can be found. As the inflation of the donor increases monotonically with increasing mass loss, the binary chop algorithm is used to optimise mass loss, similarly to optimising $f_{\rm spot}$.
To sample the error distribution of observed masses and radii, random samples were drawn from the mass and radius posterior distribution for each observation. The mean and standard deviation of the corresponding mass loss rates is reported.
